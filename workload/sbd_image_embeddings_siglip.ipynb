{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c5ee88e",
      "metadata": {
        "id": "9c5ee88e"
      },
      "source": [
        "# Video Shot Boundary Detection with SigLip2 Embeddings\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/everettVT/daft-video-embeddings/blob/main/workload/sbd_image_embeddings_siglip.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "51232d51",
      "metadata": {
        "id": "51232d51"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"daft[huggingface]\" transformers numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc274ca",
      "metadata": {
        "id": "8bc274ca"
      },
      "outputs": [],
      "source": [
        "\n",
        "T, H, W, C = 16, 288, 288, 3\n",
        "ROW_LIMIT = 2048\n",
        "\n",
        "\n",
        "# Chi-Squared SBD Params\n",
        "CHSQ_HISTOGRAM_BINS = 32\n",
        "CHSQ_THRESHOLD = 0.3\n",
        "CHSQ_MIN_SHOT_DURATION = 0.5 # seconds\n",
        "\n",
        "PATHS = [\n",
        "    \"https://www.youtube.com/watch?v=WAsmZJ2kff0\", # GPU Pipeline Optimization Explained\n",
        "    \"https://www.youtube.com/watch?v=BLcKDQRTFKY\", # Wrangle PDFs with Custom UDFs\n",
        "    \"https://www.youtube.com/watch?v=Qnw6059ddgE\", # Data and AI Processing at Scale\n",
        "    \"https://www.youtube.com/watch?v=eYXDSuNpKTk\", # Life after Apache Spark\n",
        "    \"https://www.youtube.com/watch?v=3JWrg1DitaA\", # Scaling Data Processing and ML Training with Daft + Ray\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c545f4",
      "metadata": {
        "id": "65c545f4"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "from daft.functions import embed_image\n",
        "from daft import col, lit, DataType as dt\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79448414",
      "metadata": {
        "id": "79448414"
      },
      "outputs": [],
      "source": [
        "df_frames = daft.read_video_frames(\n",
        "    PATHS,\n",
        "    image_height=H,\n",
        "    image_width=W,\n",
        ").limit(ROW_LIMIT).collect() # Materialize a few frames so we don't re-read from YT\n",
        "df_frames.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15688fdb",
      "metadata": {
        "id": "15688fdb"
      },
      "source": [
        "### Generate SigLip2 Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbdcdfb",
      "metadata": {
        "id": "adbdcdfb"
      },
      "outputs": [],
      "source": [
        "df_emb = df_frames.with_column(\"emb_siglip2_base_patch_16_512\", embed_image(df_frames[\"data\"], model_name=\"google/siglip2-base-patch16-512\", provider=\"transformers\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi-Squared Shot Boundary Detection"
      ],
      "metadata": {
        "id": "BEx9ydOEfqpV"
      },
      "id": "BEx9ydOEfqpV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625672d9",
      "metadata": {
        "id": "625672d9"
      },
      "outputs": [],
      "source": [
        "@daft.func()\n",
        "def histogram(data: daft.Image, bins: int, range: tuple[float, float]) -> dt.tensor(dt.int64(), shape=(3, HISTOGRAM_BINS)):\n",
        "    flat = np.asarray(data).reshape(-1, 3)\n",
        "    hist = np.zeros((3, bins), dtype=np.int64)\n",
        "    for i in range(3):\n",
        "        h, _ = np.histogram(flat[:, i], bins=bins, range=range)\n",
        "        hist[i] = h.astype(np.int64, copy=False)\n",
        "    return hist\n",
        "\n",
        "\n",
        "@daft.func(return_dtype=dt.list(dt.int64()))\n",
        "def detect_sbd_chsq_hysteresis(\n",
        "    hists: list[np.ndarray],\n",
        "    indices: list[int],\n",
        "    pts: list[int],\n",
        "    high_threshold: float = 0.4,\n",
        "    low_threshold: float = 0.25,\n",
        "    min_shot_duration: float = 0.5,  # seconds\n",
        "    ) -> list[int]:\n",
        "\n",
        "    if len(hists) < 2:\n",
        "        return []\n",
        "\n",
        "    # Validate thresholds\n",
        "    if not (low_threshold < high_threshold):\n",
        "        # Swap to enforce proper ordering if misconfigured\n",
        "        low_threshold, high_threshold = min(low_threshold, high_threshold), max(low_threshold, high_threshold)\n",
        "\n",
        "    # Convert and sort by frame index\n",
        "    h_arr = [np.asarray(h, dtype=np.float32) for h in hists]\n",
        "    idx_arr = np.asarray(indices, dtype=np.int64)\n",
        "    pts_arr = np.asarray(pts, dtype=np.int64)\n",
        "    order = np.argsort(idx_arr)\n",
        "    h_arr = [h_arr[i] for i in order]\n",
        "    idx_arr = idx_arr[order]\n",
        "    pts_arr = pts_arr[order]\n",
        "\n",
        "    # Compute per-transition chi-squared distances\n",
        "    eps = 1e-8\n",
        "    dists = []\n",
        "    for i in range(1, len(h_arr)):\n",
        "        h1 = h_arr[i-1]\n",
        "        h2 = h_arr[i]\n",
        "        if h1.ndim == 1:\n",
        "            h1 = h1[None, :]\n",
        "        if h2.ndim == 1:\n",
        "            h2 = h2[None, :]\n",
        "        h1n = h1 / (np.sum(h1, axis=1, keepdims=True) + eps)\n",
        "        h2n = h2 / (np.sum(h2, axis=1, keepdims=True) + eps)\n",
        "        num = (h1n - h2n) ** 2\n",
        "        den = h1n + h2n + eps\n",
        "        chisq_per_channel = 0.5 * np.sum(num / den, axis=1)\n",
        "        d = float(np.mean(chisq_per_channel))\n",
        "        dists.append(d)\n",
        "    dists = np.asarray(dists, dtype=np.float32)\n",
        "\n",
        "    # Hysteresis scan: choose the local peak within a high-threshold crossing, end when below low-threshold\n",
        "    boundaries: list[int] = []\n",
        "    min_us = int(min_shot_duration * 1_000_000.0)\n",
        "    last_boundary_pts = None\n",
        "\n",
        "    inside_cluster = False\n",
        "    peak_dist = -1.0\n",
        "    peak_i = -1\n",
        "\n",
        "    for i in range(1, len(idx_arr)):\n",
        "        d = dists[i-1]\n",
        "        if not inside_cluster:\n",
        "            if d >= high_threshold:\n",
        "                inside_cluster = True\n",
        "                peak_dist = d\n",
        "                peak_i = i\n",
        "        else:\n",
        "            # Track peak while in cluster\n",
        "            if d > peak_dist:\n",
        "                peak_dist = d\n",
        "                peak_i = i\n",
        "            # Exit when we drop below low threshold; commit the peak as boundary\n",
        "            if d <= low_threshold:\n",
        "                cand_pts = int(pts_arr[peak_i])\n",
        "                if last_boundary_pts is None or (cand_pts - last_boundary_pts) >= min_us:\n",
        "                    boundaries.append(int(idx_arr[peak_i]))\n",
        "                    last_boundary_pts = cand_pts\n",
        "                inside_cluster = False\n",
        "                peak_dist = -1.0\n",
        "                peak_i = -1\n",
        "\n",
        "    # If we ended still inside a cluster, commit the peak\n",
        "    if inside_cluster and peak_i >= 0:\n",
        "        cand_pts = int(pts_arr[peak_i])\n",
        "        if last_boundary_pts is None or (cand_pts - last_boundary_pts) >= min_us:\n",
        "            boundaries.append(int(idx_arr[peak_i]))\n",
        "\n",
        "    return boundaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922c2b26",
      "metadata": {
        "id": "922c2b26"
      },
      "outputs": [],
      "source": [
        "df_shots = (\n",
        "    df_emb\n",
        "    .with_column(\"histogram\", histogram(df_frames[\"data\"], bins=CHSQ_HISTOGRAM_BINS))\n",
        "    .with_column(\"shot_index\", (col(\"frame_time\") / daft.lit(CHSQ_MIN_SHOT_DURATION)).cast(dt.int64()))\n",
        "    .sort(\"frame_index\")\n",
        "    .groupby(\"path\", \"show_index\")\n",
        "    .agg_list(\"frame_index\", \"histogram\", \"emb_siglip2_base_patch_16_512\") #  Omit image data\n",
        "    .with_column(\"shot_boundaries\", detect_sbd_chsq(col(\"frame_index\"), col(\"histogram\"), col(\"emb_siglip2_base_patch_16_512\")))\n",
        ")\n",
        "df_clips.show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6376d4",
      "metadata": {
        "id": "ff6376d4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3f8d30",
      "metadata": {
        "id": "5e3f8d30"
      },
      "outputs": [],
      "source": [
        "df_sbd"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}