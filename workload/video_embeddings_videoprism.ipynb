{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Prepare environment\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Fetch VideoPrism repository if Python does not know about it and install\n",
        "# dependencies needed for this notebook.\n",
        "if not os.path.exists(\"videoprism_repo\"):\n",
        "  !git clone --quiet --branch=main --depth=1 \\\n",
        "     https://github.com/everettVT/videoprism.git videoprism_repo\n",
        "  os.chdir('./videoprism_repo')\n",
        "  !pip install .\n",
        "  os.chdir('..')\n",
        "\n",
        "# Append VideoPrism code to Python import path.\n",
        "if \"videoprism_repo\" not in sys.path:\n",
        "  sys.path.append(\"videoprism_repo\")"
      ],
      "metadata": {
        "id": "kyUr_6bI8i1w"
      },
      "id": "kyUr_6bI8i1w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"daft>=0.6.1\""
      ],
      "metadata": {
        "id": "ryv0FnDI9ry_"
      },
      "id": "ryv0FnDI9ry_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e51f7fc",
      "metadata": {
        "id": "2e51f7fc"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "from daft import col, DataType as dt\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.extend import backend\n",
        "import tensorflow as tf\n",
        "from videoprism import models as vp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fb88b8",
      "metadata": {
        "id": "00fb88b8"
      },
      "source": [
        "- B: batch size (number of videos in a batch).\n",
        "- T: number of frames per video clip (typically 16).\n",
        "- N: tokens per frame (for 288×288 with 18×18 patches → 16×16 = 256).\n",
        "- D: embedding dimension (Base: 768; Large: 1024).\n",
        "\n",
        "Video-text model returns:\n",
        "- video_embeddings: [B, D] (global video embeddings).\n",
        "- text_embeddings: [B, D] (global text embeddings).\n",
        "- Optional: frame_embeddings [B, T, D]; tokens [B, T×N, D]\n",
        "\n",
        "Retrieval:\n",
        "- cosine similarity reduces to dot product since outputs are L2-normalized.\n",
        "- For a single video vs K texts: [1, D] @ [K, D]^T → [1, K]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2b10eb",
      "metadata": {
        "id": "5a2b10eb"
      },
      "outputs": [],
      "source": [
        "PATHS = [\"/Users/everett-founder/Movies/digitlism.mp4\"]\n",
        "B, T, H, W, C = 2, 16, 288, 288, 3\n",
        "MODEL_NAME = 'videoprism_lvt_public_v1_base' # or 'videoprism_lvt_public_v1_large'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9375a9b",
      "metadata": {
        "id": "d9375a9b"
      },
      "outputs": [],
      "source": [
        "df_frames = daft.read_video_frames(\n",
        "    PATHS,\n",
        "    image_height=H,\n",
        "    image_width=W,\n",
        ")\n",
        "df_frames.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5682fe50",
      "metadata": {
        "id": "5682fe50"
      },
      "source": [
        "### Sampling Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c764b7f1",
      "metadata": {
        "id": "c764b7f1"
      },
      "outputs": [],
      "source": [
        "@daft.func()\n",
        "def normalize(image: np.ndarray) -> dt.tensor(dt.float32()):\n",
        "    return np.asarray(image).astype(np.float32) / 255.0\n",
        "\n",
        "df_norm = df_frames.with_column(\"image_tensor\", normalize(col(\"data\")))\n",
        "df_norm.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6833b4",
      "metadata": {
        "id": "0e6833b4"
      },
      "outputs": [],
      "source": [
        "df_grouped = (\n",
        "    df_frames\n",
        "    .with_column(\"group_index\", df_frames[\"frame_index\"] // T)\n",
        "    .groupby(\"path\", \"group_index\")\n",
        "    .agg_list(\"data\", \"frame_index\")\n",
        ")\n",
        "df_grouped.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cb62e2",
      "metadata": {
        "id": "96cb62e2"
      },
      "source": [
        "### Stack, Normalize, and Cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9e4282",
      "metadata": {
        "id": "3a9e4282"
      },
      "outputs": [],
      "source": [
        "@daft.func(return_dtype=dt.tensor(dt.float32(), shape=(1, 16,288, 288, 3)))\n",
        "def stack_clip(frames: list[np.ndarray], indices: list[int], clip_size: int):\n",
        "    \"\"\"Stacks a list of frames into a single numpy array\n",
        "\n",
        "    Args:\n",
        "        frames: List[T] of (H,W,3) float32\n",
        "        indices: List[T] of int\n",
        "\n",
        "    Returns:\n",
        "        (1,T,H,W,3) float32 in [0,1]\n",
        "\n",
        "    In a parallel/distributed groupby, a pre-group sort isn’t guaranteed\n",
        "    to survive aggregation order; partitions can concatenate in\n",
        "    non-deterministic order. Additionally, the image dtype is natively a\n",
        "    list[uint8], so we need to cast to float32 before normalizing from\n",
        "    [0,255] to [0,1].\n",
        "\n",
        "    Steps:\n",
        "    1. Aggregate both image_tensor and frame_index.\n",
        "    2. Sort by frame_index inside the group-level UDF, then stack.\n",
        "    3. Normalize and cast in one step.\n",
        "    4. Add a batch dimension and return.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Don't assume frames are sorted already:\n",
        "    order = np.argsort(np.asarray(indices))\n",
        "\n",
        "    # Convert Daft Image to np.ndarray\n",
        "    def to_np(x):\n",
        "        if hasattr(x, \"to_numpy\"):\n",
        "            return x.to_numpy()          # Daft Image -> np.ndarray (H,W,C) uint8\n",
        "        return np.asarray(x)\n",
        "\n",
        "    # Sort frames by frame_index\n",
        "    frames_sorted = [to_np(frames[i]) for i in order]\n",
        "\n",
        "    # Ensure Tails are padded with duplicates\n",
        "    if len(order) < clip_size:\n",
        "        frames_sorted.extend([frames_sorted[-1]] * (clip_size - len(order)))\n",
        "\n",
        "    # Stack, Normalize, and Cast in one step\n",
        "    x = np.stack(frames_sorted[:clip_size], axis=0).astype(np.float32) / 255.0 # (T,H,W,3) float32 in [0,1]\n",
        "\n",
        "    return x[None, ...] # [1,T,H,W,C] where T=clip_size\n",
        "\n",
        "df_clips = df_grouped.with_column(\"clip\", stack_clip(df_grouped[\"data\"], df_grouped[\"frame_index\"], clip_size=NUM_FRAMES))\n",
        "df_clips.show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c0dd8a",
      "metadata": {
        "id": "88c0dd8a"
      },
      "outputs": [],
      "source": [
        "@daft.udf(\n",
        "    return_dtype = dt.list(dt.embedding(dt.float32(), 768)),\n",
        "    batch_size=B, # clips per batch (tune for throughput)\n",
        ")\n",
        "class VideoPrismVideoUDF:\n",
        "    def __init__(self, model_name: str = \"videoprism_lvt_public_v1_base\"):\n",
        "        from videoprism import models as vp\n",
        "        self.model = vp.get_model(model_name)\n",
        "        self.params = vp.load_pretrained_weights(model_name)\n",
        "        self.text_tokenizer = vp.load_text_tokenizer('c4_en')\n",
        "\n",
        "        @jax.jit\n",
        "        def vf_b(x):  # [B,T,288,288,3] -> [B,D]\n",
        "            v, _, _ = self.model.apply(self.params, x, None, None, train=False)\n",
        "            return v\n",
        "        @jax.jit\n",
        "        def vf_1(x):  # [1,T,288,288,3] -> [1,D]\n",
        "            v, _, _ = self.model.apply(self.params, x, None, None, train=False)\n",
        "            return v\n",
        "\n",
        "        self.vf_b = vf_b\n",
        "        self.vf_1 = vf_1\n",
        "\n",
        "        # Warmup both\n",
        "        _ = self.vf_b(jnp.zeros((B, T, H, W, C), jnp.float32)).block_until_ready()\n",
        "        _ = self.vf_1(jnp.zeros((1, T, H, W, C), jnp.float32)).block_until_ready()\n",
        "\n",
        "    def __call__(self,\n",
        "        clips: list[np.ndarray],\n",
        "    ):\n",
        "        n = len(clips)\n",
        "        if n == B:\n",
        "            # Batch Inference\n",
        "            xb = jnp.stack(clips, axis=0) # [B,T,H,W,C]\n",
        "            v = self.vf_b(jnp.asarray(xb)) # [B,768]\n",
        "            return [v[i] for i in range(B)]\n",
        "        else:\n",
        "            return [self.vf_1(clip) for clip in clips]\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a5712b",
      "metadata": {
        "id": "e2a5712b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}