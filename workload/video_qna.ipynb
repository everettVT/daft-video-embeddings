{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5MSHFjLg4OhX",
      "metadata": {
        "id": "5MSHFjLg4OhX"
      },
      "source": [
        "# Video Q/A with Videoprism and Parakeet-v3\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/everettVT/daft-video-embeddings/blob/main/workload/video_embeddings_videoprism.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Videoprism is a general-purpose video encoder designed to tackle a wide spectrum of video understanding tasks, including classification localization, retrieval, captioning, and question answering.\n",
        "\n",
        "Parakeet is a 600-million-parameter multilingual automatic speech recognition (ASR) model designed for high-throughput speech-to-text transcription.\n",
        "\n",
        "In this notebook, we will explore how to leverage these foundational models to generate video and text embeddings from youtube or any video file.\n",
        "\n",
        "Video processing requires us to extract both image and audio frames, which can then use to generate embeddings. In this use case we will be transcribing the audio to text segments so we can perform RAG Q/A against both the visual and spoken content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyUr_6bI8i1w",
      "metadata": {
        "id": "kyUr_6bI8i1w"
      },
      "outputs": [],
      "source": [
        "# @title Prepare environment\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Fetch VideoPrism repository if Python does not know about it and install\n",
        "# dependencies needed for this notebook.\n",
        "if not os.path.exists(\"videoprism_repo\"):\n",
        "  !git clone --quiet --branch=main --depth=1 \\\n",
        "     https://github.com/everettVT/videoprism.git videoprism_repo\n",
        "  os.chdir('./videoprism_repo')\n",
        "  !pip install .\n",
        "  os.chdir('..')\n",
        "\n",
        "# Append VideoPrism code to Python import path.\n",
        "if \"videoprism_repo\" not in sys.path:\n",
        "  sys.path.append(\"videoprism_repo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ryv0FnDI9ry_",
      "metadata": {
        "id": "ryv0FnDI9ry_"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"daft>=0.6.1\" av yt-dlp \"jax[cuda12]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e51f7fc",
      "metadata": {
        "id": "2e51f7fc"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "from daft import col, DataType as dt\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.extend import backend\n",
        "import tensorflow as tf\n",
        "from videoprism import models as vp\n",
        "print(jax.devices())    # should list a CUDA device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fb88b8",
      "metadata": {
        "id": "00fb88b8"
      },
      "source": [
        "### Define Parameters\n",
        "\n",
        "Tensor Dimensions:\n",
        "- B: batch size (number of videos in a batch).\n",
        "- T: number of frames per video clip (typically 16).\n",
        "- N: tokens per frame (for 288×288 with 18×18 patches → 16×16 = 256).\n",
        "- D: embedding dimension (Base: 768; Large: 1024).\n",
        "\n",
        "VideoPrism supports video+text inputs and returns:\n",
        "- video_embeddings: [B, D] (global video embeddings).\n",
        "- text_embeddings: [B, D] (global text embeddings).\n",
        "- Optional: frame_embeddings [B, T, D]; tokens [B, T×N, D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2b10eb",
      "metadata": {
        "id": "5a2b10eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "B, T, H, W, C = 2, 16, 288, 288, 3\n",
        "ROW_LIMIT = 2048\n",
        "VIDEO_DIR = f\"{os.getcwd()}/videos\" # Overwrite with desired\n",
        "YT_PLAYLIST = \"https://youtube.com/playlist?list=PL3Q1vFKgSohNO4mbMKo5xccOsYWISUlou&si=yaV4YcH1qOj6vO2h\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12fac79d",
      "metadata": {
        "id": "12fac79d"
      },
      "source": [
        "### Download first 5 videos from your favorite youtube playlist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd videos && yt-dlp -I 1:6 {YT_PLAYLIST}"
      ],
      "metadata": {
        "id": "2R7MdQ3E1-M9"
      },
      "id": "2R7MdQ3E1-M9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2406c3c6",
      "metadata": {
        "id": "2406c3c6"
      },
      "source": [
        "### Discover video files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a74110",
      "metadata": {
        "id": "e0a74110"
      },
      "outputs": [],
      "source": [
        "from daft.functions import file\n",
        "\n",
        "df_files = daft.from_glob_path(VIDEO_DIR).with_column(\"file\", file(col(\"path\")))\n",
        "\n",
        "df_files.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b41e25b",
      "metadata": {
        "id": "9b41e25b"
      },
      "source": [
        "Retrieve Metadata from file headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a840b1b",
      "metadata": {
        "id": "0a840b1b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import av\n",
        "from fractions import Fraction\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "def _duration_seconds(container: av.container.input.InputContainer, video_stream: Optional[av.video.stream.VideoStream]) -> Optional[float]:\n",
        "    # container.duration is in microseconds (or None)\n",
        "    if container.duration and container.duration > 0:\n",
        "        return container.duration / 1_000_000.0\n",
        "    # fallback to stream-based duration if present\n",
        "    if video_stream and video_stream.duration and video_stream.time_base:\n",
        "        try:\n",
        "            return float(video_stream.duration * video_stream.time_base)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def probe_basic_video_meta(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000) -> Dict[str, Optional[float | int]]:\n",
        "    \"\"\"\n",
        "    Returns {'width', 'height', 'fps', 'frame_count'} using only container/stream headers.\n",
        "    Frame count is exact if the stream exposes nb_frames; otherwise estimated via duration * fps.\n",
        "    \"\"\"\n",
        "    options = {\"probesize\": str(probesize), \"analyzeduration\": str(analyzeduration_us)}\n",
        "    with file:\n",
        "        with av.open(file, mode=\"r\", options=options, metadata_encoding=\"utf-8\") as container:\n",
        "            vs = next((s for s in container.streams if s.type == \"video\"), None)\n",
        "            if not vs:\n",
        "                return {\"width\": None, \"height\": None, \"fps\": None, \"frame_count\": None}\n",
        "\n",
        "            width = getattr(vs, \"width\", None)\n",
        "            height = getattr(vs, \"height\", None)\n",
        "            time_base = getattr(vs, \"time_base\", None)\n",
        "            fps = getattr(vs, \"average_rate\", None) or getattr(vs, \"guessed_rate\", None)\n",
        "\n",
        "            nb_frames = getattr(vs, \"frames\", None)\n",
        "            if not nb_frames or nb_frames <= 0:\n",
        "                dur = _duration_seconds(container, vs)\n",
        "                nb_frames = int(round(dur * fps)) if (dur and fps) else None\n",
        "\n",
        "            return {\n",
        "                \"width\": width,\n",
        "                \"height\": height,\n",
        "                \"fps\": float(fps),\n",
        "                \"frame_count\": nb_frames,\n",
        "                \"time_base\": float(time_base),\n",
        "            }\n",
        "\n",
        "@daft.func(return_dtype = dt.struct({\n",
        "    \"width\": dt.int32(),\n",
        "    \"height\": dt.int32(),\n",
        "    \"fps\": dt.float64(),\n",
        "    \"frame_count\": dt.int32(),\n",
        "    \"time_base\": dt.float64(),\n",
        "}))\n",
        "def get_video_metadata(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000):\n",
        "\n",
        "    metadata = probe_basic_video_meta(file, probesize, analyzeduration_us)\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6e7513",
      "metadata": {
        "id": "fd6e7513"
      },
      "outputs": [],
      "source": [
        "df_meta = df_files.with_column(\"metadata\", get_video_metadata(df_files[\"file\"])).collect()\n",
        "df_meta.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d07b13",
      "metadata": {
        "id": "f6d07b13"
      },
      "outputs": [],
      "source": [
        "@daft.func(return_dtype=dt.list(dt.float32()))\n",
        "def clip_frame_time(clip_indices: list[int], time_base: float, fps: float) -> list[int]:\n",
        "    return [float(i) * time_base for i in clip_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ae1aef",
      "metadata": {
        "id": "16ae1aef"
      },
      "outputs": [],
      "source": [
        "# plan clips\n",
        "df_clip_plan = (\n",
        "    df_meta\n",
        "    .with_column(\"clip_indices\",\n",
        "        col(\"metadata\")[\"frame_count\"]\n",
        "        .apply(lambda n: list(range(0, n)), return_dtype=dt.list(dt.int32()))\n",
        "        .list.chunk(size=T)\n",
        "    )\n",
        "    .explode(\"clip_indices\")\n",
        "    .with_column(\"clip_time_base\", clip_frame_time(col(\"clip_indices\"), col(\"metadata\")[\"time_base\"], col(\"metadata\")[\"fps\"]))\n",
        ")\n",
        "df_clip_plan.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84241731",
      "metadata": {
        "id": "84241731"
      },
      "source": [
        "### Stream Video Frames"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_to_rgb_float32(frame: av.VideoStream, w: int, h: int, interp: str = None) -> np.ndarray:\n",
        "\n",
        "    frame = frame.to_ndarray(\n",
        "        width=w,\n",
        "        height=h,\n",
        "        format=\"rgb24\",\n",
        "        interpolation=interp,\n",
        "    ).astype(np.float32) / 255.0\n",
        "\n",
        "    return frame"
      ],
      "metadata": {
        "id": "1tZKBttdmsoR"
      },
      "id": "1tZKBttdmsoR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Experiment with an example:\n",
        "my_file = \"/content/videos/Why Your Image Processing Pipeline Keeps Running Out of Memory [4CGQ-c7iivg].mkv\"\n",
        "\n",
        "\n",
        "def seek_video_frames(file, start_sec, num_frames, W, H, interp):\n",
        "    with av.open(file) as container:\n",
        "        vs = container.streams.video[0]\n",
        "        vs.thread_type = \"AUTO\"\n",
        "\n",
        "        # 1) Compute seek offset in stream ticks\n",
        "        ts = int(start_sec / float(vs.time_base))  # seconds -> ticks\n",
        "\n",
        "        # 2) Seek to keyframe <= start_sec\n",
        "        container.seek(ts, stream=vs, any_frame=False, backward=True)\n",
        "\n",
        "        # 3) New decode loop; drop until PTS >= start_sec\n",
        "        out = np.empty((num_frames, H, W, 3), dtype=np.float32)\n",
        "        got = 0\n",
        "        target = start_sec\n",
        "        eps = 1e-6\n",
        "\n",
        "        for frame in container.decode(video=0):\n",
        "            if frame.pts is None:\n",
        "                continue\n",
        "            t = frame.pts * float(vs.time_base)\n",
        "            if t + eps < target:\n",
        "                continue  # not reached start yet\n",
        "            # 4) Collect frames\n",
        "            arr = frame_to_rgb_float32(frame, w=W, h=H, interp=interp)\n",
        "            out[got] = arr\n",
        "            got += 1\n",
        "            if got == num_frames:\n",
        "                break\n",
        "\n",
        "        # If fewer than requested frames exist, trim\n",
        "        return out[:got]\n",
        "\n",
        "stack = seek_video_frames(my_file, 20, 16, 288, 288, None)\n",
        "\n",
        "# Display the first frame from the stack\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(stack[i])\n",
        "    plt.axis('off')  # Hide axes"
      ],
      "metadata": {
        "id": "zJ0LdLVTmQLM"
      },
      "id": "zJ0LdLVTmQLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc899616",
      "metadata": {
        "id": "fc899616"
      },
      "outputs": [],
      "source": [
        "@daft.func()\n",
        "def yield_clip_stack(\n",
        "    file: daft.File,\n",
        "    start_sec: float,\n",
        "    num_frames: int = 16,\n",
        "    w: int = 288,\n",
        "    h: int = 288,\n",
        "    interp: str = None\n",
        "    ) -> dt.tensor(dt.float32(), shape=(T, H, W, 3)):\n",
        "\n",
        "    with file:\n",
        "        with av.open(file) as container:\n",
        "            vs = container.streams.video[0]\n",
        "            vs.thread_type = \"AUTO\"\n",
        "\n",
        "            # 1) Compute seek offset in stream ticks\n",
        "            ts = int(start_sec / float(vs.time_base))  # seconds -> ticks\n",
        "\n",
        "            # 2) Seek to keyframe <= start_sec\n",
        "            container.seek(ts, stream=vs, any_frame=False, backward=True)\n",
        "\n",
        "            # 3) New decode loop; drop until PTS >= start_sec\n",
        "            empty_arr = np.empty((num_frames, H, W, 3), dtype=np.float32)\n",
        "            out = empty_arr.copy()\n",
        "            got = 0\n",
        "            target = start_sec\n",
        "            eps = 1e-6\n",
        "\n",
        "            for frame in container.decode(video=0):\n",
        "                if frame.pts is None:\n",
        "                    continue\n",
        "                t = frame.pts * float(vs.time_base)\n",
        "                if t + eps < target:\n",
        "                    continue  # not reached start yet\n",
        "\n",
        "                # 4) Collect frames\n",
        "                arr = frame_to_rgb_float32(frame, w=W, h=H, interp=interp)\n",
        "                out[got] = arr\n",
        "                got += 1\n",
        "                # Check if we have collected enough frames or reached the end of the video\n",
        "                if got == num_frames or (frame.pts * float(vs.time_base) + eps >= (vs.duration * vs.time_base) if vs.duration and vs.time_base else False):\n",
        "                    break\n",
        "\n",
        "\n",
        "            # If fewer than requested frames exist, fill with zeros\n",
        "            if got < num_frames:\n",
        "                out[got:] = 0\n",
        "\n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4826cf4a",
      "metadata": {
        "id": "4826cf4a"
      },
      "outputs": [],
      "source": [
        "df_clips = (\n",
        "    df_clip_plan\n",
        "    .with_column(\"clip\", yield_clip_stack(\n",
        "            col(\"file\"),\n",
        "            col(\"clip_indices\").list.get(0),\n",
        "            num_frames=T,\n",
        "            w=W,\n",
        "            h=H\n",
        "        )\n",
        "    )\n",
        ")\n",
        "df_clips.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f73ead73",
      "metadata": {
        "id": "f73ead73"
      },
      "source": [
        "### Define Inference Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c0dd8a",
      "metadata": {
        "id": "88c0dd8a"
      },
      "outputs": [],
      "source": [
        "B = 8\n",
        "@daft.udf(\n",
        "    return_dtype = dt.embedding(dt.float32(), 768),\n",
        "    batch_size=B, # clips per batch (tune for throughput)\n",
        "    num_gpus=1,\n",
        ")\n",
        "class VideoPrismVideoUDF:\n",
        "    def __init__(self, model_name: str = \"videoprism_lvt_public_v1_base\"):\n",
        "        \"for 'videoprism_lvt_public_v1_large', set T = 8\"\n",
        "\n",
        "        from videoprism import models as vp\n",
        "        self.model = vp.get_model(model_name)\n",
        "        self.params = vp.load_pretrained_weights(model_name)\n",
        "\n",
        "        @jax.jit\n",
        "        def vf_b(clips):  # [B,T,288,288,3] -> [B,D]\n",
        "            v, _, _ = self.model.apply(\n",
        "                self.params,\n",
        "                clips,\n",
        "                None,\n",
        "                None,\n",
        "                train=False\n",
        "            )\n",
        "            return v\n",
        "\n",
        "        self.vf_b = vf_b\n",
        "\n",
        "        # Warmup both\n",
        "        _ = self.vf_b(jnp.zeros((B, T, H, W, C), jnp.float32)).block_until_ready()\n",
        "\n",
        "    def __call__(self,\n",
        "        clips: list[np.ndarray], # List[T,H,W,C] of len B\n",
        "    ):\n",
        "        # Batch Inference\n",
        "        xb = jnp.stack(clips, axis=0)    # [B,T,H,W,3]\n",
        "        video_embeddings = self.vf_b(xb) # [B,768]\n",
        "        np_emb = np.asarray(video_embeddings)  # Back to NumPy\n",
        "        return [np_emb[i].tolist() for i in range(len(np_emb))]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gNx0eBzCQV7R",
      "metadata": {
        "id": "gNx0eBzCQV7R"
      },
      "source": [
        "Recommended GPU Batch sizes:\n",
        "- T4 (B = 2)\n",
        "- L4 (B = 2)\n",
        "- A100 (B = 24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a5712b",
      "metadata": {
        "id": "e2a5712b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "df_video_embs = (\n",
        "    df_clips\n",
        "    .with_column(\"video_embeddings\", VideoPrismVideoUDF(col(\"clip\")))\n",
        ")\n",
        "df_video_embs.limit(B*2).collect()\n",
        "print(f\"Video Embeddings processed {B} clips, {T} frames in {time.time()-start} sec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nb5h4MpIFbn0",
      "metadata": {
        "id": "nb5h4MpIFbn0"
      },
      "outputs": [],
      "source": [
        "df_video_embs.select(\"group_index\",\"video_embeddings\", \"clip\").count_rows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xcEeeY8DRRQq",
      "metadata": {
        "id": "xcEeeY8DRRQq"
      },
      "source": [
        "Extract Audio from Video, Transcribe and Embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655d2593",
      "metadata": {
        "id": "655d2593"
      },
      "outputs": [],
      "source": [
        "import av\n",
        "from av.audio.resampler import AudioResampler\n",
        "\n",
        "@daft.func()\n",
        "def extract_audio_frames_into_numpy_arrays(file: daft.File) -> np.ndarray:\n",
        "\n",
        "    container = av.open(file)\n",
        "    resampler = AudioResampler(format='s16', layout='mono', rate=16000)\n",
        "\n",
        "    chunks = []\n",
        "    try:\n",
        "        for frame in container.decode(audio=0):\n",
        "            # Resample to desired SR/mono/PCM16; result can be a frame or list of frames\n",
        "            res = resampler.resample(frame)\n",
        "            frames = res if isinstance(res, (list, tuple)) else [res]\n",
        "\n",
        "            for f in frames:\n",
        "                arr = f.to_ndarray()  # typically (channels, samples) or (samples,)\n",
        "                arr = np.asarray(arr)\n",
        "\n",
        "                # Flatten to 1-D mono\n",
        "                if arr.ndim == 2:\n",
        "                    # (1, N) or (N, 1) → (N,)\n",
        "                    if arr.shape[0] == 1:\n",
        "                        arr = arr[0]\n",
        "                    elif arr.shape[1] == 1:\n",
        "                        arr = arr[:, 0]\n",
        "                    else:\n",
        "                        # Unexpected multi-channel after mono resample: average as fallback\n",
        "                        arr = arr.mean(axis=0)\n",
        "                elif arr.ndim > 2:\n",
        "                    arr = arr.reshape(-1)\n",
        "\n",
        "                # Convert PCM16 → float32 in [-1, 1]\n",
        "                if arr.dtype != np.float32:\n",
        "                    arr = (arr.astype(np.float32) / 32768.0).clip(-1.0, 1.0)\n",
        "\n",
        "                chunks.append(arr)\n",
        "    finally:\n",
        "        container.close()\n",
        "\n",
        "    if not chunks:\n",
        "        return np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "    audio = np.concatenate(chunks, axis=0).astype(np.float32, copy=False)\n",
        "    return audio\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddce0e7",
      "metadata": {
        "id": "3ddce0e7"
      },
      "outputs": [],
      "source": [
        "@daft.udf(return_dtype = dt.string())\n",
        "class ParakeetTranscribeUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        import nemo.collections.asr as nemo_asr\n",
        "        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
        "        self.asr_model.change_attention_model(\n",
        "            self_attention_model=\"rel_pos_local_attn\",\n",
        "            att_context_size=[context_size, context_size]\n",
        "        )\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio)\n",
        "        texts = [o.text for o in outputs]\n",
        "        return texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e1bc0c",
      "metadata": {
        "id": "a0e1bc0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Parakeet Transcribe with Timestamps\n",
        "@daft.udf(return_dtype = dt.struct({\n",
        "    \"segment\": dt.list(dt.struct({\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "}))\n",
        "class ParakeetTranscribeTimestampsUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        import nemo.collections.asr as nemo_asr\n",
        "        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
        "        self.asr_model.change_attention_model(\n",
        "            self_attention_model=\"rel_pos_local_attn\",\n",
        "            att_context_size=[context_size, context_size]\n",
        "        )\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio, timestamps=True)   # No public flag to emit only segments\n",
        "        return [o.timestamp[\"segment\"] for o in outputs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qBF-KLZ4H-et",
      "metadata": {
        "id": "qBF-KLZ4H-et"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "@daft.udf(\n",
        "    return_dtype = dt.embedding(dt.float32(), 768),\n",
        "    batch_size=B, # clips per batch (tune for throughput)\n",
        "    num_gpus=1,\n",
        ")\n",
        "class VideoPrismTextUDF:\n",
        "    def __init__(self, model_name: str = \"videoprism_lvt_public_v1_base\"):\n",
        "        from videoprism import models as vp\n",
        "        self.model = vp.get_model(model_name)\n",
        "        self.params = vp.load_pretrained_weights(model_name)\n",
        "        self.text_tokenizer = vp.load_text_tokenizer('c4_en')\n",
        "\n",
        "        @jax.jit\n",
        "        def vf_b(text_ids, text_paddings):  # [B,T,288,288,3] -> [B,D]\n",
        "            _, t, _ = self.model.apply(self.params, None, text_ids, text_paddings, train=False)\n",
        "            return t # text embeddings\n",
        "\n",
        "        self.vf_b = vf_b\n",
        "\n",
        "        # Warmup both\n",
        "        text_ids, text_paddings = vp.tokenize_texts(self.text_tokenizer, [\"Hello\", \"World\"])\n",
        "        _ = self.vf_b(None, text_ids, text_paddings).block_until_ready()\n",
        "\n",
        "    def __call__(self,\n",
        "        prompts: list[str], # List[T,H,W,C] of len B\n",
        "    ):\n",
        "        # Batch Inference\n",
        "        text_ids, text_paddings = vp.tokenize_texts(self.text_tokenizer, prompts)\n",
        "        text_embeddings = self.vf_b(text_ids, text_paddings)\n",
        "\n",
        "        return text_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e51240",
      "metadata": {
        "id": "84e51240"
      },
      "source": [
        "# Using daft.File instead of read_video_frames()\n",
        "\n",
        "Video data, as a critical type of multimodal data, uniquely integrates visual, audio, and temporal dimensions, inherently fusing spatial (image-based) and temporal information. It has been widely adopted across domains including short-video platforms, live streaming, public security, healthcare, and autonomous driving.\n",
        "\n",
        "Given the large volume of video data, most processing paradigms typically involve streaming-based reading and processing to minimize memory footprint. This distinguishes it from image data, which generally requires full initial loading into memory prior to processing.\n",
        "\n",
        "Thus, when introducing the Video data type into Daft, it should avoid storing the entire dataset in memory. Drawing inspiration from the File data type, we can either store merely a URL reference to the video data or directly utilize the underlying data structure of the File data type as its internal representation.\n",
        "\n",
        "Beyond the core content of video data, it is critical to extract key metadata to facilitate subsequent filtering of target videos prior to processing. Videos encompass extensive metadata, such as frame count, resolution (height/width), time base, duration, pixel format, bit rate, codec name, and profile, among others. However, incorporating all such metadata into the Video data type is impractical from a memory efficiency standpoint. Instead, we prioritize including only essential metadata fields—specifically frame count, height, width, and FPS. Additional metadata can be dynamically retrieved during video processing as needed.\n",
        "\n",
        "daft.read_video_frames is a very convenient API for reading video frames, also including recognize/filter the key frames and resize frame, it's very helpful for most common cases, but it might more focus on the case that reading key frames, there are some other cases might need a Video data type and native functions(even though we can used customized UDF to achieve it.), feel free to discuss:\n",
        "support using different algorithms to reading/extracting accurate key frames, e.g. difference, optical flow, the default behavior of PyAV to extract key frame is based on I-frame the use the native encoding metadata pict_type='I'\n",
        "Besides extracting/reading key frames, there are other use case about video, e.g. split video by key frame, extract audio from video, etc. it's better to add these functions on Video or file data type instead of adding new API for each use cases.\n",
        "From performance perspective, it's better to use rust library to handle video processing logical as much as possible, e.g. ffmpeg-next, even though most tools are based on ffmepg\n",
        "filter/pushdown video data based on thier metadata before processing them.\n",
        "\n",
        "\n",
        "R Conner Howell\n",
        ":daft:  Aug 28th at 11:07 AM\n",
        "I agree. The idea behind the \"File\" type was to start with a wrapper of the appropriate python file-like protocols, then to further type into VideoFile, AudioFile, PdfFile, etc. — each of which having their own domain-specific methods such a read_frames, read_channels, read_pages etc. respectively. As you have also pointed out, this enables daft to implement this functionality in Rust as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75d2b9b",
      "metadata": {
        "id": "b75d2b9b"
      },
      "outputs": [],
      "source": [
        "df_file = daft.from_glob_path(\"~/Movies/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d938a8d",
      "metadata": {
        "id": "6d938a8d"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.from_glob_path(\"s3://bucket/videos/\")\n",
        "\n",
        "# Convert path to video directly from utf8 data type to video type. Daft should support convert from utf8 and file data type both.\n",
        "df = df.with_column(\"video\", video(col(\"path\")))\n",
        "\n",
        "# Filter video by video metadata.\n",
        "df = df.filter((df[\"width\"] > 1024) & (df[\"height\"] > 576) & (df[\"frames\"] > 100)))\n",
        "\n",
        "# Extract the key frames, the `key_frames` function will streaming read video data\n",
        "# and extract multiple key frames, the data type of each frame is FixedShapeImage. The `key_frames` might add more parameters to indicate what's the image mode of key frames.\n",
        "# TODO consider whether to include some metadata for key frame to compatible with daft.read_video_frames\n",
        "df = df.with_column(\"key_frames\", key_frames(col(\"video\"), method= \"I_frame\").explode(\"key_frames\")\n",
        "\n",
        "# Save the key frames as a dataset.\n",
        "df.select(\"path\", \"key_frames\").write_lance(\"key_frames_dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4691d0da",
      "metadata": {
        "id": "4691d0da"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be80a58",
      "metadata": {
        "id": "3be80a58"
      },
      "outputs": [],
      "source": [
        "# Parakeet Transcribe with Timestamps\n",
        "@daft.udf(return_dtype = dt.struct({\n",
        "    \"word\": dt.list(dt.struct({\n",
        "        \"word\": dt.string(),\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "    \"segment\": dt.list(dt.struct({\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "    \"char\": dt.list(dt.struct({\n",
        "        \"char\": dt.string(),\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "}))\n",
        "class ParakeetTranscribeTimestampsUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        import nemo.collections.asr as nemo_asr\n",
        "        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
        "        self.asr_model.change_attention_model(\n",
        "            self_attention_model=\"rel_pos_local_attn\",\n",
        "            att_context_size=[context_size, context_size]\n",
        "        )\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio, timestamps=True)   # No public flag to emit only segments\n",
        "        return [o.timestamp for o in outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ab5926",
      "metadata": {
        "id": "19ab5926"
      },
      "outputs": [],
      "source": [
        "class DiarizationSortFormerUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        from nemo.collections.asr.models import SortformerEncLabelModel\n",
        "        self.diar_model = SortformerEncLabelModel.from_pretrained(\"nvidia/diar_streaming_sortformer_4spk-v2\")\n",
        "        self.diar_model.eval() # Switch to inference mode\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio, timestamps=True)   # No public flag to emit only segments\n",
        "        return [o.timestamp for o in outputs]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}