{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5MSHFjLg4OhX",
      "metadata": {
        "id": "5MSHFjLg4OhX"
      },
      "source": [
        "# Video Q/A with Videoprism and Parakeet-v3\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/everettVT/daft-video-embeddings/blob/main/friction/UNFILTERED_Pt2_videoprism_parakeet.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Videoprism is a general-purpose video encoder designed to tackle a wide spectrum of video understanding tasks, including classification localization, retrieval, captioning, and question answering.\n",
        "\n",
        "Parakeet is a 600-million-parameter multilingual automatic speech recognition (ASR) model designed for high-throughput speech-to-text transcription.\n",
        "\n",
        "In this notebook, we will explore how to leverage these foundational models to generate video and text embeddings from youtube or any video file.\n",
        "\n",
        "Video processing requires us to extract both image and audio frames, which can then use to generate embeddings. In this use case we will be transcribing the audio to text segments so we can perform RAG Q/A against both the visual and spoken content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyUr_6bI8i1w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyUr_6bI8i1w",
        "outputId": "de529f1e-0796-484a-992d-c72310a5e9b9"
      },
      "outputs": [],
      "source": [
        "# @title Prepare environment\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Fetch VideoPrism repository if Python does not know about it and install\n",
        "# dependencies needed for this notebook.\n",
        "if not os.path.exists(\"videoprism_repo\"):\n",
        "  !git clone --quiet --branch=main --depth=1 \\\n",
        "     https://github.com/everettVT/videoprism.git videoprism_repo\n",
        "  os.chdir('./videoprism_repo')\n",
        "  !pip install .\n",
        "  os.chdir('..')\n",
        "\n",
        "# Append VideoPrism code to Python import path.\n",
        "if \"videoprism_repo\" not in sys.path:\n",
        "  sys.path.append(\"videoprism_repo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ryv0FnDI9ry_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ryv0FnDI9ry_",
        "outputId": "7c5e90eb-20a5-4fb7-ec3f-c329d1637232"
      },
      "outputs": [],
      "source": [
        "!pip install \"daft>=0.6.1\" av yt-dlp \"jax[cuda12]\" \"nemo_toolkit[asr]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2e51f7fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e51f7fc",
        "outputId": "fc6b1d1c-a57e-41ec-ebee-9a72f0d01682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CpuDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "import daft\n",
        "from daft import col, DataType as dt\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.extend import backend\n",
        "import tensorflow as tf\n",
        "from videoprism import models as vp\n",
        "print(jax.devices())    # should list a CUDA device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fb88b8",
      "metadata": {
        "id": "00fb88b8"
      },
      "source": [
        "### Define Parameters\n",
        "\n",
        "Tensor Dimensions:\n",
        "- B: batch size (number of videos in a batch).\n",
        "- T: number of frames per video clip (typically 16).\n",
        "- N: tokens per frame (for 288Ã—288 with 18Ã—18 patches â†’ 16Ã—16 = 256).\n",
        "- D: embedding dimension (Base: 768; Large: 1024).\n",
        "\n",
        "VideoPrism supports video+text inputs and returns:\n",
        "- video_embeddings: [B, D] (global video embeddings).\n",
        "- text_embeddings: [B, D] (global text embeddings).\n",
        "- Optional: frame_embeddings [B, T, D]; tokens [B, TÃ—N, D]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5a2b10eb",
      "metadata": {
        "id": "5a2b10eb"
      },
      "outputs": [],
      "source": [
        "B, T, H, W, C = 24, 16, 288, 288, 3\n",
        "ROW_LIMIT = 2048\n",
        "PWD = !pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e33_yh36P8Of",
      "metadata": {
        "id": "e33_yh36P8Of"
      },
      "source": [
        "Download first 5 videos from your favorite youtube playlist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQZDKavnP6WU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQZDKavnP6WU",
        "outputId": "379396e3-de59-409a-c5ff-8d0a4320eb90"
      },
      "outputs": [],
      "source": [
        "!mkdir videos && cd videos && yt-dlp -I 1:6 https://www.youtube.com/playlist?list=PL3Q1vFKgSohNO4mbMKo5xccOsYWISUlou"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CCcbC7e3QUpC",
      "metadata": {
        "id": "CCcbC7e3QUpC"
      },
      "source": [
        "Discover the file paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "w9xNHuhYQTeV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "w9xNHuhYQTeV",
        "outputId": "f3552239-20a7-4bd8-9793-b1e9ee93c223"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <style>\n",
              "        .dashboard-container {\n",
              "            display: flex;\n",
              "            gap: 20px;\n",
              "            max-width: 100%;\n",
              "            height: 100%;\n",
              "        }\n",
              "        .table-container {\n",
              "            flex: 1;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .side-pane {\n",
              "            width: 35%;\n",
              "            max-height: 500px;\n",
              "            border: 1px solid;\n",
              "            border-radius: 4px;\n",
              "            padding: 15px;\n",
              "            display: none;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .side-pane.visible {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "        }\n",
              "        .side-pane-header {\n",
              "            display: flex;\n",
              "            justify-content: space-between;\n",
              "            align-items: center;\n",
              "            margin-bottom: 10px;\n",
              "            padding-bottom: 10px;\n",
              "            border-bottom: 1px solid;\n",
              "        }\n",
              "        .side-pane-title {\n",
              "            font-weight: bold;\n",
              "        }\n",
              "        .close-button {\n",
              "            cursor: pointer;\n",
              "        }\n",
              "        .side-pane-content {\n",
              "            word-wrap: break-word;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .dataframe td.clickable {\n",
              "            cursor: pointer;\n",
              "            transition: background-color 0.2s;\n",
              "        }\n",
              "        .dataframe td.clickable:hover {\n",
              "            opacity: 0.8;\n",
              "        }\n",
              "        .dataframe td.clickable.selected {\n",
              "            opacity: 0.6;\n",
              "        }\n",
              "        </style>\n",
              "        <div class=\"dashboard-container\">\n",
              "            <div class=\"table-container\">\n",
              "        <div id=\"dataframe-a1f3596d-317f-484e-91de-3bcb7a9c3f30\"><table class=\"dataframe\" style=\"table-layout: fixed; min-width: 100%\">\n",
              "<thead><tr><th style=\"text-wrap: nowrap; width: calc(100vw / 4); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">path<br />Utf8</th><th style=\"text-wrap: nowrap; width: calc(100vw / 4); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; width: calc(100vw / 4); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">num_rows<br />Int64</th><th style=\"text-wrap: nowrap; width: calc(100vw / 4); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">file<br />File</th></tr></thead>\n",
              "<tbody>\n",
              "<tr><td data-row=\"0\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">file:///Users/everett-founder/Movies/Running.mp4</div></td><td data-row=\"0\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">132936424</div></td><td data-row=\"0\" data-col=\"2\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">None</div></td><td data-row=\"0\" data-col=\"3\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">File(Reference(\"file:///Users/everett-founder/Movies/Running.mp4\", None))</div></td></tr>\n",
              "<tr><td data-row=\"1\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">file:///Users/everett-founder/Movies/digitlism.mp4</div></td><td data-row=\"1\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">9176010</div></td><td data-row=\"1\" data-col=\"2\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">None</div></td><td data-row=\"1\" data-col=\"3\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">File(Reference(\"file:///Users/everett-founder/Movies/digitlism.mp4\", None))</div></td></tr>\n",
              "<tr><td data-row=\"2\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">file:///Users/everett-founder/Movies/StructureOutputsWorkloadWalkthrough.mp4</div></td><td data-row=\"2\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">153828067</div></td><td data-row=\"2\" data-col=\"2\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">None</div></td><td data-row=\"2\" data-col=\"3\"><div style=\"text-align:left; width: calc(100vw / 4); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">File(Reference(\"file:///Users/everett-founder/Movies/StructureOutputsWorkloadWalkthrough.mp4\", None))</div></td></tr>\n",
              "</tbody>\n",
              "</table></div>\n",
              "            </div>\n",
              "            <div class=\"side-pane\" id=\"side-pane-a1f3596d-317f-484e-91de-3bcb7a9c3f30\">\n",
              "                <div class=\"side-pane-header\">\n",
              "                    <div class=\"side-pane-title\" id=\"side-pane-title-a1f3596d-317f-484e-91de-3bcb7a9c3f30\">Cell Details</div>\n",
              "                    <button class=\"close-button\" id=\"close-button-a1f3596d-317f-484e-91de-3bcb7a9c3f30\">Ã—</button>\n",
              "                </div>\n",
              "                <div class=\"side-pane-content\" id=\"side-pane-content-a1f3596d-317f-484e-91de-3bcb7a9c3f30\">\n",
              "                    <p style=\"font-style: italic;\">Click on a cell to view its full content</p>\n",
              "                </div>\n",
              "            </div>\n",
              "        </div>\n",
              "    \n",
              "        <script>\n",
              "        (function() {\n",
              "            const serverUrl = 'http://127.0.0.1:3238';\n",
              "            const dfId = 'a1f3596d-317f-484e-91de-3bcb7a9c3f30';\n",
              "            const dataframeElement = document.getElementById('dataframe-' + dfId);\n",
              "            const cells = dataframeElement ? dataframeElement.querySelectorAll('td') : [];\n",
              "            const sidePane = document.getElementById('side-pane-' + dfId);\n",
              "            const sidePaneTitle = document.getElementById('side-pane-title-' + dfId);\n",
              "            const sidePaneContent = document.getElementById('side-pane-content-' + dfId);\n",
              "            const closeButton = document.getElementById('close-button-' + dfId);\n",
              "            let selectedCell = null;\n",
              "\n",
              "            function closeSidePane(paneId) {\n",
              "                const pane = document.getElementById('side-pane-' + paneId);\n",
              "                if (pane) {\n",
              "                    pane.classList.remove('visible');\n",
              "                    if (selectedCell) {\n",
              "                        selectedCell.classList.remove('selected');\n",
              "                        selectedCell = null;\n",
              "                    }\n",
              "                }\n",
              "            }\n",
              "\n",
              "            function showSidePane(row, col, content) {\n",
              "                sidePaneTitle.textContent = 'Cell (' + row + ', ' + col + ')';\n",
              "                sidePaneContent.innerHTML = content;\n",
              "                sidePane.classList.add('visible');\n",
              "            }\n",
              "\n",
              "            function showLoadingContent() {\n",
              "                sidePaneContent.innerHTML = '<div style=\"text-align:center; padding:20px;\"><span style=\"font-style:italic\">Loading full content...</span></div>';\n",
              "            }\n",
              "\n",
              "            // Add event listener for close button\n",
              "            if (closeButton) {\n",
              "                closeButton.addEventListener('click', function() {\n",
              "                    closeSidePane(dfId);\n",
              "                });\n",
              "            }\n",
              "\n",
              "            cells.forEach((cell) => {\n",
              "                // Skip cells that do not have data-row and data-col attributes (e.g., ellipsis row)\n",
              "                const rowAttr = cell.getAttribute('data-row');\n",
              "                const colAttr = cell.getAttribute('data-col');\n",
              "                if (rowAttr === null || colAttr === null) return;\n",
              "\n",
              "                const row = parseInt(rowAttr);\n",
              "                const col = parseInt(colAttr);\n",
              "                cell.classList.add('clickable');\n",
              "\n",
              "                cell.onclick = function() {\n",
              "                    // Remove selection from previously selected cell\n",
              "                    if (selectedCell && selectedCell !== cell) {\n",
              "                        selectedCell.classList.remove('selected');\n",
              "                    }\n",
              "\n",
              "                    // Toggle selection for current cell\n",
              "                    if (selectedCell === cell) {\n",
              "                        cell.classList.remove('selected');\n",
              "                        selectedCell = null;\n",
              "                        closeSidePane(dfId);\n",
              "                        return;\n",
              "                    } else {\n",
              "                        cell.classList.add('selected');\n",
              "                        selectedCell = cell;\n",
              "                    }\n",
              "\n",
              "                    // Show the side pane immediately\n",
              "                    showSidePane(row, col, '');\n",
              "\n",
              "                    // Set a timeout to show loading content after 1 second\n",
              "                    const loadingTimeout = setTimeout(() => {\n",
              "                        showLoadingContent();\n",
              "                    }, 100);\n",
              "\n",
              "                    // Fetch the cell content\n",
              "                    fetch(serverUrl + '/api/dataframes/' + dfId + '/cell?row=' + row + '&col=' + col)\n",
              "                        .then(response => response.json())\n",
              "                        .then(data => {\n",
              "                            clearTimeout(loadingTimeout);\n",
              "                            showSidePane(row, col, data.value);\n",
              "                        })\n",
              "                        .catch(err => {\n",
              "                            clearTimeout(loadingTimeout);\n",
              "                            // Get the original cell content from the table\n",
              "                            const cell = selectedCell;\n",
              "                            if (cell) {\n",
              "                                const originalContent = cell.innerHTML;\n",
              "                                showSidePane(row, col, originalContent);\n",
              "                            }\n",
              "                        });\n",
              "                };\n",
              "            });\n",
              "        })();\n",
              "        </script>\n",
              "        \n",
              "<small>(Showing first 3 of 3 rows)</small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from daft.functions import file\n",
        "df_files = daft.from_glob_path(f\"{PWD[0]}/videos\").with_column(\"file\", file(col(\"path\")))\n",
        "\n",
        "df_files.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nNv4NAv7T-Wz",
      "metadata": {
        "id": "nNv4NAv7T-Wz"
      },
      "source": [
        "## Read Video File Metadata for early Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "i7RMTjXsXB4w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7RMTjXsXB4w",
        "outputId": "dee624e9-876d-4b5f-ce98-d6e78d430db4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import av\n",
        "from fractions import Fraction\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "def _to_float_rate(rate: Optional[Fraction]) -> Optional[float]:\n",
        "    try:\n",
        "        return float(rate) if rate else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _duration_seconds(container: av.container.input.InputContainer, video_stream: Optional[av.video.stream.VideoStream]) -> Optional[float]:\n",
        "    # container.duration is in microseconds (or None)\n",
        "    if container.duration and container.duration > 0:\n",
        "        return container.duration / 1_000_000.0\n",
        "    # fallback to stream-based duration if present\n",
        "    if video_stream and video_stream.duration and video_stream.time_base:\n",
        "        try:\n",
        "            return float(video_stream.duration * video_stream.time_base)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def probe_video_header_with_pyav(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Probe only headers to get cheap metadata. Does not decode frames.\n",
        "\n",
        "    - probesize: demuxer read budget (e.g., '32k', '64k', '1M').\n",
        "    - analyzeduration_us: analysis budget in microseconds (e.g., 200_000 = 0.2s).\n",
        "    \"\"\"\n",
        "    # Open read-only with constrained probe budgets\n",
        "    options = {\n",
        "        \"probesize\": str(probesize),\n",
        "        \"analyzeduration\": str(analyzeduration_us),\n",
        "    }\n",
        "    with file:\n",
        "        with av.open(file, mode=\"r\", options=options, metadata_encoding=\"utf-8\") as container:\n",
        "            # Choose the first video stream if present\n",
        "            video_stream = next((s for s in container.streams if s.type == \"video\"), None)\n",
        "            audio_stream = next((s for s in container.streams if s.type == \"audio\"), None)\n",
        "\n",
        "            # General/container\n",
        "            meta: Dict[str, Any] = {\n",
        "                \"path\": file,\n",
        "                \"name\": os.path.basename(file),\n",
        "                \"size_bytes\": os.path.getsize(file),\n",
        "                \"format_name\": getattr(container.format, \"name\", None),\n",
        "                \"format_long_name\": getattr(container.format, \"long_name\", None),\n",
        "                \"tags\": dict(container.metadata or {}),\n",
        "                \"bit_rate\": getattr(container, \"bit_rate\", None),\n",
        "                \"duration_seconds\": _duration_seconds(container, video_stream),\n",
        "            }\n",
        "\n",
        "            # Video\n",
        "            if video_stream:\n",
        "                fps = _to_float_rate(getattr(video_stream, \"average_rate\", None)) or \\\n",
        "                    _to_float_rate(getattr(video_stream, \"guessed_rate\", None))\n",
        "\n",
        "                rotation = 0\n",
        "                try:\n",
        "                    # Commonly stored in metadata as 'rotate'\n",
        "                    if \"rotate\" in (video_stream.metadata or {}):\n",
        "                        rotation = int(video_stream.metadata[\"rotate\"])\n",
        "                except Exception:\n",
        "                    rotation = 0\n",
        "\n",
        "                meta[\"video\"] = {\n",
        "                    \"codec\": getattr(video_stream.codec_context, \"name\", None),\n",
        "                    \"profile\": getattr(video_stream.codec_context, \"profile\", None),\n",
        "                    \"width\": getattr(video_stream, \"width\", None),\n",
        "                    \"height\": getattr(video_stream, \"height\", None),\n",
        "                    \"fps\": fps,\n",
        "                    \"pix_fmt\": getattr(video_stream, \"pix_fmt\", None),\n",
        "                    \"sample_aspect_ratio\": str(getattr(video_stream, \"sample_aspect_ratio\", None)) if getattr(video_stream, \"sample_aspect_ratio\", None) else None,\n",
        "                    \"display_aspect_ratio\": str(getattr(video_stream, \"display_aspect_ratio\", None)) if getattr(video_stream, \"display_aspect_ratio\", None) else None,\n",
        "                    \"rotation\": rotation,\n",
        "                    \"time_base\": float(video_stream.time_base) if getattr(video_stream, \"time_base\", None) else None,\n",
        "                    \"duration_seconds_stream\": (float(video_stream.duration * video_stream.time_base)\n",
        "                                                if getattr(video_stream, \"duration\", None) and getattr(video_stream, \"time_base\", None) else None),\n",
        "                    \"bit_rate\": getattr(video_stream, \"bit_rate\", None),\n",
        "                }\n",
        "\n",
        "            # Audio\n",
        "            if audio_stream:\n",
        "                meta[\"audio\"] = {\n",
        "                    \"codec\": getattr(audio_stream.codec_context, \"name\", None),\n",
        "                    \"profile\": getattr(audio_stream.codec_context, \"profile\", None),\n",
        "                    \"sample_rate\": getattr(audio_stream, \"rate\", None),\n",
        "                    \"channels\": getattr(audio_stream, \"channels\", None),\n",
        "                    \"channel_layout\": str(getattr(audio_stream, \"layout\", None)) if getattr(audio_stream, \"layout\", None) else None,\n",
        "                    \"time_base\": float(audio_stream.time_base) if getattr(audio_stream, \"time_base\", None) else None,\n",
        "                    \"bit_rate\": getattr(audio_stream, \"bit_rate\", None),\n",
        "                }\n",
        "\n",
        "        return meta\n",
        "\n",
        "# Example usage:\n",
        "# info = probe_video_header_with_pyav(\"/path/to/video.mp4\", probesize=\"64k\", analyzeduration_us=200_000)\n",
        "# print(info)\n",
        "\n",
        "metadata_schema = dt.struct({\n",
        "    \"path\": dt.string(),\n",
        "    \"name\": dt.string(),\n",
        "    \"size_bytes\": dt.int64(),\n",
        "    \"format_name\": dt.string(),\n",
        "    \"format_long_name\": dt.string(),\n",
        "    \"tags\": dt.struct({\n",
        "        \"major_brand\": dt.string(),\n",
        "        \"minor_version\": dt.string(),\n",
        "        \"compatible_brands\": dt.string(),\n",
        "        \"encoder\": dt.string(),\n",
        "    }),\n",
        "    \"bit_rate\": dt.int64(),\n",
        "    \"duration_seconds\": dt.float64(),\n",
        "    \"video\": dt.struct({\n",
        "        \"codec\": dt.string(),\n",
        "        \"profile\": dt.string(),\n",
        "        \"width\": dt.int32(),\n",
        "        \"height\": dt.int32(),\n",
        "        \"fps\": dt.float64(),\n",
        "        \"pix_fmt\": dt.string(),\n",
        "        \"sample_aspect_ratio\": dt.string(),\n",
        "        \"display_aspect_ratio\": dt.string(),\n",
        "        \"rotation\": dt.int32(),\n",
        "        \"time_base\": dt.float64(),\n",
        "        \"duration_seconds_stream\": dt.float64(),\n",
        "        \"bit_rate\": dt.int64(),\n",
        "    }),\n",
        "    \"audio\": dt.struct({\n",
        "        \"codec\": dt.string(),\n",
        "        \"profile\": dt.string(),\n",
        "        \"sample_rate\": dt.int32(),\n",
        "        \"channels\": dt.int32(),\n",
        "        \"channel_layout\": dt.string(),\n",
        "        \"time_base\": dt.float64(),\n",
        "        \"bit_rate\": dt.int64(),\n",
        "    }),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919e2cdd",
      "metadata": {},
      "source": [
        "TRY PASSING DAFT.FILE DIRECTLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "fTY4uaNIT9xF",
      "metadata": {
        "id": "fTY4uaNIT9xF"
      },
      "outputs": [],
      "source": [
        "@daft.func()\n",
        "def read_video_metadata(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000) -> metadata_schema:\n",
        "    \n",
        "    metadata = probe_video_header_with_pyav(file, probesize, analyzeduration_us)\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "Unvx48pwVTxS",
      "metadata": {
        "id": "Unvx48pwVTxS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/everett-founder/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/dashboard/__init__.py:91: UserWarning: Failed to broadcast metrics over http://127.0.0.1:3238/api/queries: HTTP Error 400: Bad Request\n",
            "  warnings.warn(f\"Failed to broadcast metrics over {url}: {e}\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e52915dd8da4fda9d53086909a0f605",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ InMemorySource: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e1204cb65a04a6cac4f96daedd9a92d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ Filter: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4c264598fe04ab19e1966c6a67589d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ Project: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "582277fa6ccf47f29ce4ea8eee7b77fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ UDF read_video_metadata: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error when running pipeline node UDF read_video_metadata\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not PathFile",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_meta = \u001b[43mdf_files\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_column\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_video_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_files\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df_meta.show(\u001b[32m5\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/dataframe/dataframe.py:4013\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self, num_preview_rows)\u001b[39m\n\u001b[32m   3982\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes the entire DataFrame and materializes the results.\u001b[39;00m\n\u001b[32m   3983\u001b[39m \n\u001b[32m   3984\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4010\u001b[39m \u001b[33;03m    (Showing first 3 of 3 rows)\u001b[39;00m\n\u001b[32m   4011\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4012\u001b[39m \u001b[38;5;28mself\u001b[39m._broadcast_query_plan()\n\u001b[32m-> \u001b[39m\u001b[32m4013\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_materialize_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4015\u001b[39m dataframe_len = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/dataframe/dataframe.py:3975\u001b[39m, in \u001b[36mDataFrame._materialize_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3973\u001b[39m context = get_context()\n\u001b[32m   3974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3975\u001b[39m     \u001b[38;5;28mself\u001b[39m._result_cache = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_or_create_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3976\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m   3977\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/runners/native_runner.py:66\u001b[39m, in \u001b[36mNativeRunner.run\u001b[39m\u001b[34m(self, builder)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, builder: LogicalPlanBuilder) -> PartitionCacheEntry:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     results = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.run_iter(builder))\n\u001b[32m     68\u001b[39m     result_pset = LocalPartitionSet()\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/runners/native_runner.py:99\u001b[39m, in \u001b[36mNativeRunner.run_iter\u001b[39m\u001b[34m(self, builder, results_buffer_size)\u001b[39m\n\u001b[32m     92\u001b[39m executor = NativeExecutor()\n\u001b[32m     93\u001b[39m results_gen = executor.run(\n\u001b[32m     94\u001b[39m     plan,\n\u001b[32m     95\u001b[39m     {k: v.values() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._part_set_cache.get_all_partition_sets().items()},\n\u001b[32m     96\u001b[39m     daft_execution_config,\n\u001b[32m     97\u001b[39m     results_buffer_size,\n\u001b[32m     98\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m results_gen\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/execution/native_executor.py:42\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdaft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartitioning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocalMaterializedResult\n\u001b[32m     39\u001b[39m psets_mp = {\n\u001b[32m     40\u001b[39m     part_id: [part.micropartition()._micropartition \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts] \u001b[38;5;28;01mfor\u001b[39;00m part_id, parts \u001b[38;5;129;01min\u001b[39;00m psets.items()\n\u001b[32m     41\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLocalMaterializedResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMicroPartition\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pymicropartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_physical_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpsets_mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdaft_execution_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/udf/row_wise.py:126\u001b[39m, in \u001b[36m__call_func\u001b[39m\u001b[34m(fn, original_args, evaluated_args)\u001b[39m\n\u001b[32m    123\u001b[39m new_args = [evaluated_args.pop(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Expression) \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m    124\u001b[39m new_kwargs = {key: (evaluated_args.pop(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Expression) \u001b[38;5;28;01melse\u001b[39;00m arg) \u001b[38;5;28;01mfor\u001b[39;00m key, arg \u001b[38;5;129;01min\u001b[39;00m kwargs.items()}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mread_video_metadata\u001b[39m\u001b[34m(file, probesize, analyzeduration_us)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@daft\u001b[39m.func()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_video_metadata\u001b[39m(file: daft.File, probesize: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m64k\u001b[39m\u001b[33m\"\u001b[39m, analyzeduration_us: \u001b[38;5;28mint\u001b[39m = \u001b[32m200_000\u001b[39m) -> metadata_schema:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     metadata = \u001b[43mprobe_video_header_with_pyav\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobesize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalyzeduration_us\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mprobe_video_header_with_pyav\u001b[39m\u001b[34m(file, probesize, analyzeduration_us)\u001b[39m\n\u001b[32m     40\u001b[39m audio_stream = \u001b[38;5;28mnext\u001b[39m((s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m container.streams \u001b[38;5;28;01mif\u001b[39;00m s.type == \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# General/container\u001b[39;00m\n\u001b[32m     43\u001b[39m meta: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m: file,\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msize_bytes\u001b[39m\u001b[33m\"\u001b[39m: os.path.getsize(file),\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(container.format, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_long_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(container.format, \u001b[33m\"\u001b[39m\u001b[33mlong_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(container.metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbit_rate\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mgetattr\u001b[39m(container, \u001b[33m\"\u001b[39m\u001b[33mbit_rate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mduration_seconds\u001b[39m\u001b[33m\"\u001b[39m: _duration_seconds(container, video_stream),\n\u001b[32m     52\u001b[39m }\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Video\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m video_stream:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:142\u001b[39m, in \u001b[36mbasename\u001b[39m\u001b[34m(p)\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not PathFile"
          ]
        }
      ],
      "source": [
        "df_meta = df_files.with_column(\"metadata\", read_video_metadata(df_files[\"file\"])).collect()\n",
        "df_meta.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2aaca32b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@daft.func()\n",
        "def read_video_metadata_2(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000) -> metadata_schema:\n",
        "    \n",
        "    metadata = probe_video_header_with_pyav(str(file), probesize, analyzeduration_us)\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c2e25a03",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/everett-founder/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/dashboard/__init__.py:91: UserWarning: Failed to broadcast metrics over http://127.0.0.1:3238/api/queries: HTTP Error 400: Bad Request\n",
            "  warnings.warn(f\"Failed to broadcast metrics over {url}: {e}\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "835ad4acb0224d59b1f0486cad0381c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ InMemorySource: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "202d039d27924ad49b64e3fa38f05be6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ Filter: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7361c97bce6144c591b5f1534f4665b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ Project: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b10ac50ad98a456dacc2f378396e437a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸ—¡ï¸ ðŸŸ UDF read_video_metadata_2: 00:00 "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error when running pipeline node UDF read_video_metadata_2\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'File(file:///Users/everett-founder/Movies/Running.mp4)'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_meta = \u001b[43mdf_files\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_column\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_video_metadata_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_files\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df_meta.show(\u001b[32m5\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/dataframe/dataframe.py:4013\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self, num_preview_rows)\u001b[39m\n\u001b[32m   3982\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes the entire DataFrame and materializes the results.\u001b[39;00m\n\u001b[32m   3983\u001b[39m \n\u001b[32m   3984\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4010\u001b[39m \u001b[33;03m    (Showing first 3 of 3 rows)\u001b[39;00m\n\u001b[32m   4011\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4012\u001b[39m \u001b[38;5;28mself\u001b[39m._broadcast_query_plan()\n\u001b[32m-> \u001b[39m\u001b[32m4013\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_materialize_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4014\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4015\u001b[39m dataframe_len = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/dataframe/dataframe.py:3975\u001b[39m, in \u001b[36mDataFrame._materialize_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3973\u001b[39m context = get_context()\n\u001b[32m   3974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3975\u001b[39m     \u001b[38;5;28mself\u001b[39m._result_cache = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_or_create_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3976\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m   3977\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/runners/native_runner.py:66\u001b[39m, in \u001b[36mNativeRunner.run\u001b[39m\u001b[34m(self, builder)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, builder: LogicalPlanBuilder) -> PartitionCacheEntry:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     result_pset = LocalPartitionSet()\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/runners/native_runner.py:99\u001b[39m, in \u001b[36mNativeRunner.run_iter\u001b[39m\u001b[34m(self, builder, results_buffer_size)\u001b[39m\n\u001b[32m     92\u001b[39m executor = NativeExecutor()\n\u001b[32m     93\u001b[39m results_gen = executor.run(\n\u001b[32m     94\u001b[39m     plan,\n\u001b[32m     95\u001b[39m     {k: v.values() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._part_set_cache.get_all_partition_sets().items()},\n\u001b[32m     96\u001b[39m     daft_execution_config,\n\u001b[32m     97\u001b[39m     results_buffer_size,\n\u001b[32m     98\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m results_gen\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/execution/native_executor.py:42\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdaft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartitioning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocalMaterializedResult\n\u001b[32m     39\u001b[39m psets_mp = {\n\u001b[32m     40\u001b[39m     part_id: [part.micropartition()._micropartition \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts] \u001b[38;5;28;01mfor\u001b[39;00m part_id, parts \u001b[38;5;129;01min\u001b[39;00m psets.items()\n\u001b[32m     41\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLocalMaterializedResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMicroPartition\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pymicropartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_physical_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpsets_mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdaft_execution_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/daft/udf/row_wise.py:126\u001b[39m, in \u001b[36m__call_func\u001b[39m\u001b[34m(fn, original_args, evaluated_args)\u001b[39m\n\u001b[32m    123\u001b[39m new_args = [evaluated_args.pop(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Expression) \u001b[38;5;28;01melse\u001b[39;00m arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m    124\u001b[39m new_kwargs = {key: (evaluated_args.pop(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Expression) \u001b[38;5;28;01melse\u001b[39;00m arg) \u001b[38;5;28;01mfor\u001b[39;00m key, arg \u001b[38;5;129;01min\u001b[39;00m kwargs.items()}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mread_video_metadata_2\u001b[39m\u001b[34m(file, probesize, analyzeduration_us)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@daft\u001b[39m.func()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_video_metadata_2\u001b[39m(file: daft.File, probesize: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m64k\u001b[39m\u001b[33m\"\u001b[39m, analyzeduration_us: \u001b[38;5;28mint\u001b[39m = \u001b[32m200_000\u001b[39m) -> metadata_schema:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     metadata = \u001b[43mprobe_video_header_with_pyav\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobesize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manalyzeduration_us\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mprobe_video_header_with_pyav\u001b[39m\u001b[34m(path, probesize, analyzeduration_us)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Open read-only with constrained probe budgets\u001b[39;00m\n\u001b[32m     32\u001b[39m options = {\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprobesize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(probesize),\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33manalyzeduration\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(analyzeduration_us),\n\u001b[32m     35\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mav\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m container:\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Choose the first video stream if present\u001b[39;00m\n\u001b[32m     38\u001b[39m     video_stream = \u001b[38;5;28mnext\u001b[39m((s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m container.streams \u001b[38;5;28;01mif\u001b[39;00m s.type == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     39\u001b[39m     audio_stream = \u001b[38;5;28mnext\u001b[39m((s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m container.streams \u001b[38;5;28;01mif\u001b[39;00m s.type == \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/av/container/core.pyx:417\u001b[39m, in \u001b[36mav.container.core.open\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/av/container/core.pyx:282\u001b[39m, in \u001b[36mav.container.core.Container.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/av/container/core.pyx:302\u001b[39m, in \u001b[36mav.container.core.Container.err_check\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/git/dream/daft-video-embeddings/.venv/lib/python3.11/site-packages/av/error.pyx:424\u001b[39m, in \u001b[36mav.error.err_check\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'File(file:///Users/everett-founder/Movies/Running.mp4)'"
          ]
        }
      ],
      "source": [
        "df_meta = df_files.with_column(\"metadata\", read_video_metadata_2(df_files[\"file\"])).collect()\n",
        "df_meta.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1a21da70",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import av\n",
        "from fractions import Fraction\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "def _to_float_rate(rate: Optional[Fraction]) -> Optional[float]:\n",
        "    try:\n",
        "        return float(rate) if rate else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _duration_seconds(container: av.container.input.InputContainer, video_stream: Optional[av.video.stream.VideoStream]) -> Optional[float]:\n",
        "    # container.duration is in microseconds (or None)\n",
        "    if container.duration and container.duration > 0:\n",
        "        return container.duration / 1_000_000.0\n",
        "    # fallback to stream-based duration if present\n",
        "    if video_stream and video_stream.duration and video_stream.time_base:\n",
        "        try:\n",
        "            return float(video_stream.duration * video_stream.time_base)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def probe_basic_video_meta(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000) -> Dict[str, Optional[float | int]]:\n",
        "    \"\"\"\n",
        "    Returns {'width', 'height', 'fps', 'frame_count'} using only container/stream headers.\n",
        "    Frame count is exact if the stream exposes nb_frames; otherwise estimated via duration * fps.\n",
        "    \"\"\"\n",
        "    options = {\"probesize\": str(probesize), \"analyzeduration\": str(analyzeduration_us)}\n",
        "    with file:\n",
        "        with av.open(file, mode=\"r\", options=options, metadata_encoding=\"utf-8\") as container:\n",
        "            vs = next((s for s in container.streams if s.type == \"video\"), None)\n",
        "            if not vs:\n",
        "                return {\"width\": None, \"height\": None, \"fps\": None, \"frame_count\": None}\n",
        "\n",
        "            width = getattr(vs, \"width\", None)\n",
        "            height = getattr(vs, \"height\", None)\n",
        "            fps = _to_float_rate(getattr(vs, \"average_rate\", None)) or _to_float_rate(getattr(vs, \"guessed_rate\", None))\n",
        "\n",
        "            nb_frames = getattr(vs, \"frames\", None)\n",
        "            if not nb_frames or nb_frames <= 0:\n",
        "                dur = _duration_seconds(container, vs)\n",
        "                nb_frames = int(round(dur * fps)) if (dur and fps) else None\n",
        "\n",
        "            return {\n",
        "                \"width\": width,\n",
        "                \"height\": height,\n",
        "                \"fps\": fps,\n",
        "                \"frame_count\": nb_frames,\n",
        "            }\n",
        "\n",
        "@daft.func(return_dtype = dt.struct({\n",
        "    \"width\": dt.int32(),\n",
        "    \"height\": dt.int32(),\n",
        "    \"fps\": dt.float64(),\n",
        "    \"frame_count\": dt.int32(),\n",
        "}))\n",
        "def get_video_metadata(file: daft.File, probesize: str = \"64k\", analyzeduration_us: int = 200_000):\n",
        "\n",
        "    metadata = probe_basic_video_meta(file, probesize, analyzeduration_us)\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "72519c5f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <style>\n",
              "        .dashboard-container {\n",
              "            display: flex;\n",
              "            gap: 20px;\n",
              "            max-width: 100%;\n",
              "            height: 100%;\n",
              "        }\n",
              "        .table-container {\n",
              "            flex: 1;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .side-pane {\n",
              "            width: 35%;\n",
              "            max-height: 500px;\n",
              "            border: 1px solid;\n",
              "            border-radius: 4px;\n",
              "            padding: 15px;\n",
              "            display: none;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .side-pane.visible {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "        }\n",
              "        .side-pane-header {\n",
              "            display: flex;\n",
              "            justify-content: space-between;\n",
              "            align-items: center;\n",
              "            margin-bottom: 10px;\n",
              "            padding-bottom: 10px;\n",
              "            border-bottom: 1px solid;\n",
              "        }\n",
              "        .side-pane-title {\n",
              "            font-weight: bold;\n",
              "        }\n",
              "        .close-button {\n",
              "            cursor: pointer;\n",
              "        }\n",
              "        .side-pane-content {\n",
              "            word-wrap: break-word;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .dataframe td.clickable {\n",
              "            cursor: pointer;\n",
              "            transition: background-color 0.2s;\n",
              "        }\n",
              "        .dataframe td.clickable:hover {\n",
              "            opacity: 0.8;\n",
              "        }\n",
              "        .dataframe td.clickable.selected {\n",
              "            opacity: 0.6;\n",
              "        }\n",
              "        </style>\n",
              "        <div class=\"dashboard-container\">\n",
              "            <div class=\"table-container\">\n",
              "        <div id=\"dataframe-6c18fcd6-d4db-4de5-8fa7-772f353fe881\"><table class=\"dataframe\" style=\"table-layout: fixed; min-width: 100%\">\n",
              "<thead><tr><th style=\"text-wrap: nowrap; width: calc(100vw / 5); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">path<br />Utf8</th><th style=\"text-wrap: nowrap; width: calc(100vw / 5); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">size<br />Int64</th><th style=\"text-wrap: nowrap; width: calc(100vw / 5); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">num_rows<br />Int64</th><th style=\"text-wrap: nowrap; width: calc(100vw / 5); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">file<br />File</th><th style=\"text-wrap: nowrap; width: calc(100vw / 5); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">metadata<br />Struct[width: Int32, height: Int32, fps: Float64, frame_count: Int32]</th></tr></thead>\n",
              "<tbody>\n",
              "<tr><td data-row=\"0\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">file:///Users/everett-founder/Movies/Running.mp4</div></td><td data-row=\"0\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">132936424</div></td><td data-row=\"0\" data-col=\"2\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">None</div></td><td data-row=\"0\" data-col=\"3\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">File(Reference(\"file:///Users/everett-founder/Movies/Running.mp4\", None))</div></td><td data-row=\"0\" data-col=\"4\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">{width: 1280,<br />height: 720,<br />fps: 30,<br />frame_count: 45733,<br />}</div></td></tr>\n",
              "<tr><td data-row=\"1\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">file:///Users/everett-founder/Movies/digitlism.mp4</div></td><td data-row=\"1\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">9176010</div></td><td data-row=\"1\" data-col=\"2\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">None</div></td><td data-row=\"1\" data-col=\"3\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">File(Reference(\"file:///Users/everett-founder/Movies/digitlism.mp4\", None))</div></td><td data-row=\"1\" data-col=\"4\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">{width: 3336,<br />height: 2160,<br />fps: 60,<br />frame_count: 460,<br />}</div></td></tr>\n",
              "<tr><td data-row=\"2\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">file:///Users/everett-founder/Movies/StructureOutputsWorkloadWalkthrough.mp4</div></td><td data-row=\"2\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">153828067</div></td><td data-row=\"2\" data-col=\"2\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">None</div></td><td data-row=\"2\" data-col=\"3\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">File(Reference(\"file:///Users/everett-founder/Movies/StructureOutputsWorkloadWalkthrough.mp4\", None))</div></td><td data-row=\"2\" data-col=\"4\"><div style=\"text-align:left; width: calc(100vw / 5); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">{width: 1920,<br />height: 1080,<br />fps: 30,<br />frame_count: 17977,<br />}</div></td></tr>\n",
              "</tbody>\n",
              "</table></div>\n",
              "            </div>\n",
              "            <div class=\"side-pane\" id=\"side-pane-6c18fcd6-d4db-4de5-8fa7-772f353fe881\">\n",
              "                <div class=\"side-pane-header\">\n",
              "                    <div class=\"side-pane-title\" id=\"side-pane-title-6c18fcd6-d4db-4de5-8fa7-772f353fe881\">Cell Details</div>\n",
              "                    <button class=\"close-button\" id=\"close-button-6c18fcd6-d4db-4de5-8fa7-772f353fe881\">Ã—</button>\n",
              "                </div>\n",
              "                <div class=\"side-pane-content\" id=\"side-pane-content-6c18fcd6-d4db-4de5-8fa7-772f353fe881\">\n",
              "                    <p style=\"font-style: italic;\">Click on a cell to view its full content</p>\n",
              "                </div>\n",
              "            </div>\n",
              "        </div>\n",
              "    \n",
              "        <script>\n",
              "        (function() {\n",
              "            const serverUrl = 'http://127.0.0.1:3238';\n",
              "            const dfId = '6c18fcd6-d4db-4de5-8fa7-772f353fe881';\n",
              "            const dataframeElement = document.getElementById('dataframe-' + dfId);\n",
              "            const cells = dataframeElement ? dataframeElement.querySelectorAll('td') : [];\n",
              "            const sidePane = document.getElementById('side-pane-' + dfId);\n",
              "            const sidePaneTitle = document.getElementById('side-pane-title-' + dfId);\n",
              "            const sidePaneContent = document.getElementById('side-pane-content-' + dfId);\n",
              "            const closeButton = document.getElementById('close-button-' + dfId);\n",
              "            let selectedCell = null;\n",
              "\n",
              "            function closeSidePane(paneId) {\n",
              "                const pane = document.getElementById('side-pane-' + paneId);\n",
              "                if (pane) {\n",
              "                    pane.classList.remove('visible');\n",
              "                    if (selectedCell) {\n",
              "                        selectedCell.classList.remove('selected');\n",
              "                        selectedCell = null;\n",
              "                    }\n",
              "                }\n",
              "            }\n",
              "\n",
              "            function showSidePane(row, col, content) {\n",
              "                sidePaneTitle.textContent = 'Cell (' + row + ', ' + col + ')';\n",
              "                sidePaneContent.innerHTML = content;\n",
              "                sidePane.classList.add('visible');\n",
              "            }\n",
              "\n",
              "            function showLoadingContent() {\n",
              "                sidePaneContent.innerHTML = '<div style=\"text-align:center; padding:20px;\"><span style=\"font-style:italic\">Loading full content...</span></div>';\n",
              "            }\n",
              "\n",
              "            // Add event listener for close button\n",
              "            if (closeButton) {\n",
              "                closeButton.addEventListener('click', function() {\n",
              "                    closeSidePane(dfId);\n",
              "                });\n",
              "            }\n",
              "\n",
              "            cells.forEach((cell) => {\n",
              "                // Skip cells that do not have data-row and data-col attributes (e.g., ellipsis row)\n",
              "                const rowAttr = cell.getAttribute('data-row');\n",
              "                const colAttr = cell.getAttribute('data-col');\n",
              "                if (rowAttr === null || colAttr === null) return;\n",
              "\n",
              "                const row = parseInt(rowAttr);\n",
              "                const col = parseInt(colAttr);\n",
              "                cell.classList.add('clickable');\n",
              "\n",
              "                cell.onclick = function() {\n",
              "                    // Remove selection from previously selected cell\n",
              "                    if (selectedCell && selectedCell !== cell) {\n",
              "                        selectedCell.classList.remove('selected');\n",
              "                    }\n",
              "\n",
              "                    // Toggle selection for current cell\n",
              "                    if (selectedCell === cell) {\n",
              "                        cell.classList.remove('selected');\n",
              "                        selectedCell = null;\n",
              "                        closeSidePane(dfId);\n",
              "                        return;\n",
              "                    } else {\n",
              "                        cell.classList.add('selected');\n",
              "                        selectedCell = cell;\n",
              "                    }\n",
              "\n",
              "                    // Show the side pane immediately\n",
              "                    showSidePane(row, col, '');\n",
              "\n",
              "                    // Set a timeout to show loading content after 1 second\n",
              "                    const loadingTimeout = setTimeout(() => {\n",
              "                        showLoadingContent();\n",
              "                    }, 100);\n",
              "\n",
              "                    // Fetch the cell content\n",
              "                    fetch(serverUrl + '/api/dataframes/' + dfId + '/cell?row=' + row + '&col=' + col)\n",
              "                        .then(response => response.json())\n",
              "                        .then(data => {\n",
              "                            clearTimeout(loadingTimeout);\n",
              "                            showSidePane(row, col, data.value);\n",
              "                        })\n",
              "                        .catch(err => {\n",
              "                            clearTimeout(loadingTimeout);\n",
              "                            // Get the original cell content from the table\n",
              "                            const cell = selectedCell;\n",
              "                            if (cell) {\n",
              "                                const originalContent = cell.innerHTML;\n",
              "                                showSidePane(row, col, originalContent);\n",
              "                            }\n",
              "                        });\n",
              "                };\n",
              "            });\n",
              "        })();\n",
              "        </script>\n",
              "        \n",
              "<small>(Showing first 3 of 3 rows)</small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_meta = df_files.with_column(\"metadata\", get_video_metadata(df_files[\"file\"])).collect()\n",
        "df_meta.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xcEeeY8DRRQq",
      "metadata": {
        "id": "xcEeeY8DRRQq"
      },
      "source": [
        "Extract Audio Frames from Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655d2593",
      "metadata": {
        "id": "655d2593"
      },
      "outputs": [],
      "source": [
        "import av\n",
        "from av.audio.resampler import AudioResampler\n",
        "\n",
        "@daft.func()\n",
        "def extract_audio_frames_into_numpy_arrays(file: daft.File) -> np.ndarray:\n",
        "\n",
        "    container = av.open(file)\n",
        "    resampler = AudioResampler(format='s16', layout='mono', rate=16000)\n",
        "\n",
        "    chunks = []\n",
        "    try:\n",
        "        for frame in container.decode(audio=0):\n",
        "            # Resample to desired SR/mono/PCM16; result can be a frame or list of frames\n",
        "            res = resampler.resample(frame)\n",
        "            frames = res if isinstance(res, (list, tuple)) else [res]\n",
        "\n",
        "            for f in frames:\n",
        "                arr = f.to_ndarray()  # typically (channels, samples) or (samples,)\n",
        "                arr = np.asarray(arr)\n",
        "\n",
        "                # Flatten to 1-D mono\n",
        "                if arr.ndim == 2:\n",
        "                    # (1, N) or (N, 1) â†’ (N,)\n",
        "                    if arr.shape[0] == 1:\n",
        "                        arr = arr[0]\n",
        "                    elif arr.shape[1] == 1:\n",
        "                        arr = arr[:, 0]\n",
        "                    else:\n",
        "                        # Unexpected multi-channel after mono resample: average as fallback\n",
        "                        arr = arr.mean(axis=0)\n",
        "                elif arr.ndim > 2:\n",
        "                    arr = arr.reshape(-1)\n",
        "\n",
        "                # Convert PCM16 â†’ float32 in [-1, 1]\n",
        "                if arr.dtype != np.float32:\n",
        "                    arr = (arr.astype(np.float32) / 32768.0).clip(-1.0, 1.0)\n",
        "\n",
        "                chunks.append(arr)\n",
        "    finally:\n",
        "        container.close()\n",
        "\n",
        "    if not chunks:\n",
        "        return np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "    audio = np.concatenate(chunks, axis=0).astype(np.float32, copy=False)\n",
        "    return audio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75e10c3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "157331f6",
      "metadata": {},
      "source": [
        " Transcribe Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddce0e7",
      "metadata": {
        "id": "3ddce0e7"
      },
      "outputs": [],
      "source": [
        "@daft.udf(return_dtype = dt.string())\n",
        "class ParakeetTranscribeUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        import nemo.collections.asr as nemo_asr\n",
        "        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
        "        self.asr_model.change_attention_model(\n",
        "            self_attention_model=\"rel_pos_local_attn\",\n",
        "            att_context_size=[context_size, context_size]\n",
        "        )\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio)\n",
        "        texts = [o.text for o in outputs]\n",
        "        return texts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480e9d05",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ba01bfc3",
      "metadata": {},
      "source": [
        "Transcribe Audio with Timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e1bc0c",
      "metadata": {
        "id": "a0e1bc0c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Parakeet Transcribe with Timestamps\n",
        "@daft.udf(return_dtype = dt.struct({\n",
        "    \"segment\": dt.list(dt.struct({\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "}))\n",
        "class ParakeetTranscribeTimestampsUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        import nemo.collections.asr as nemo_asr\n",
        "        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
        "        self.asr_model.change_attention_model(\n",
        "            self_attention_model=\"rel_pos_local_attn\",\n",
        "            att_context_size=[context_size, context_size]\n",
        "        )\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio, timestamps=True)   # No public flag to emit only segments\n",
        "        return [o.timestamp[\"segment\"] for o in outputs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62adc65e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "05262fcd",
      "metadata": {},
      "source": [
        "Embed Transcription with Videoprism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qBF-KLZ4H-et",
      "metadata": {
        "id": "qBF-KLZ4H-et"
      },
      "outputs": [],
      "source": [
        "@daft.udf(\n",
        "    return_dtype = dt.embedding(dt.float32(), 768),\n",
        "    batch_size=B, # clips per batch (tune for throughput)\n",
        "    num_gpus=1,\n",
        ")\n",
        "class VideoPrismTextUDF:\n",
        "    def __init__(self, model_name: str = \"videoprism_lvt_public_v1_base\"):\n",
        "        from videoprism import models as vp\n",
        "        self.model = vp.get_model(model_name)\n",
        "        self.params = vp.load_pretrained_weights(model_name)\n",
        "        self.text_tokenizer = vp.load_text_tokenizer('c4_en')\n",
        "\n",
        "        @jax.jit\n",
        "        def vf_b(text_ids, text_paddings):  # [B,T,288,288,3] -> [B,D]\n",
        "            _, t, _ = self.model.apply(self.params, None, text_ids, text_paddings, train=False)\n",
        "            return t # text embeddings\n",
        "\n",
        "        self.vf_b = vf_b\n",
        "\n",
        "        # Warmup both\n",
        "        text_ids, text_paddings = vp.tokenize_texts(self.text_tokenizer, [\"Hello\", \"World\"])\n",
        "        _ = self.vf_b(None, text_ids, text_paddings).block_until_ready()\n",
        "\n",
        "    def __call__(self,\n",
        "        prompts: list[str], # List[T,H,W,C] of len B\n",
        "    ):\n",
        "        # Batch Inference\n",
        "        text_ids, text_paddings = vp.tokenize_texts(self.text_tokenizer, prompts)\n",
        "        text_embeddings = self.vf_b(text_ids, text_paddings)\n",
        "\n",
        "        return text_embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e51240",
      "metadata": {
        "id": "84e51240"
      },
      "source": [
        "# Using daft.File instead of read_video_frames()\n",
        "\n",
        "Video data, as a critical type of multimodal data, uniquely integrates visual, audio, and temporal dimensions, inherently fusing spatial (image-based) and temporal information. It has been widely adopted across domains including short-video platforms, live streaming, public security, healthcare, and autonomous driving.\n",
        "\n",
        "Given the large volume of video data, most processing paradigms typically involve streaming-based reading and processing to minimize memory footprint. This distinguishes it from image data, which generally requires full initial loading into memory prior to processing.\n",
        "\n",
        "Thus, when introducing the Video data type into Daft, it should avoid storing the entire dataset in memory. Drawing inspiration from the File data type, we can either store merely a URL reference to the video data or directly utilize the underlying data structure of the File data type as its internal representation.\n",
        "\n",
        "Beyond the core content of video data, it is critical to extract key metadata to facilitate subsequent filtering of target videos prior to processing. Videos encompass extensive metadata, such as frame count, resolution (height/width), time base, duration, pixel format, bit rate, codec name, and profile, among others. However, incorporating all such metadata into the Video data type is impractical from a memory efficiency standpoint. \n",
        "Instead, we prioritize including only essential metadata fieldsâ€”specifically frame count, height, width, and FPS. Additional metadata can be dynamically retrieved during video processing as needed.\n",
        "\n",
        "\n",
        "- support using different algorithms to reading/extracting accurate key frames, e.g. difference, optical flow, the default behavior of PyAV to extract key frame is based on I-frame the use the native encoding metadata pict_type='I'\n",
        "\n",
        "- Besides extracting/reading key frames, there are other use case about video, e.g. split video by key frame, extract audio from video, etc.\n",
        "\n",
        " it's better to add these functions on Video or file data type instead of adding new API for each use cases.\n",
        "\n",
        "From performance perspective, it's better to use rust library to handle video processing logical as much as possible, e.g. ffmpeg-next, even though most tools are based on ffmepg filter/pushdown video data based on thier metadata before processing them.\n",
        "\n",
        "\n",
        "\n",
        "R Conner Howell\n",
        ":daft:  Aug 28th at 11:07 AM\n",
        "I agree. The idea behind the \"File\" type was to start with a wrapper of the appropriate python file-like protocols, then to further type into VideoFile, AudioFile, PdfFile, etc. â€” each of which having their own domain-specific methods such a read_frames, read_channels, read_pages etc. respectively. As you have also pointed out, this enables daft to implement this functionality in Rust as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d938a8d",
      "metadata": {
        "id": "6d938a8d"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.from_glob_path(\"s3://bucket/videos/\")\n",
        "\n",
        "# Convert path to video directly from utf8 data type to video type. Daft should support convert from utf8 and file data type both.\n",
        "df = df.with_column(\"video\", video(col(\"path\")))\n",
        "\n",
        "# Filter video by video metadata.\n",
        "df = df.filter((df[\"width\"] > 1024) & (df[\"height\"] > 576) & (df[\"frames\"] > 100)))\n",
        "\n",
        "# Extract the key frames, the `key_frames` function will streaming read video data\n",
        "# and extract multiple key frames, the data type of each frame is FixedShapeImage. The `key_frames` might add more parameters to indicate what's the image mode of key frames.\n",
        "# TODO consider whether to include some metadata for key frame to compatible with daft.read_video_frames\n",
        "df = df.with_column(\"key_frames\", key_frames(col(\"video\"), method= \"I_frame\").explode(\"key_frames\")\n",
        "\n",
        "# Save the key frames as a dataset.\n",
        "df.select(\"path\", \"key_frames\").write_lance(\"key_frames_dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84241731",
      "metadata": {
        "id": "84241731"
      },
      "source": [
        "### Read Video Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9375a9b",
      "metadata": {
        "id": "d9375a9b"
      },
      "outputs": [],
      "source": [
        "df_video_frames = daft.read_video_frames(\n",
        "    df_files[\"path\"],\n",
        "    image_height=H,\n",
        "    image_width=W,\n",
        ").limit(ROW_LIMIT).collect()\n",
        "df_frames.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5682fe50",
      "metadata": {
        "id": "5682fe50"
      },
      "source": [
        "### Group Frames into Clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6833b4",
      "metadata": {
        "id": "0e6833b4"
      },
      "outputs": [],
      "source": [
        "df_grouped = (\n",
        "    df_video_frames\n",
        "    .with_column(\"group_index\", df_frames[\"frame_index\"] // T)\n",
        "    .groupby(\"path\", \"group_index\")\n",
        "    .agg_list(\"data\", \"frame_index\")\n",
        ")\n",
        "df_grouped.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96cb62e2",
      "metadata": {
        "id": "96cb62e2"
      },
      "source": [
        "### Stack, Normalize, and Cast Frames into Clip Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9e4282",
      "metadata": {
        "id": "3a9e4282"
      },
      "outputs": [],
      "source": [
        "@daft.func(return_dtype=dt.tensor(dt.float32(), shape=(16,288, 288, 3)))\n",
        "def stack_clip(frames: list[np.ndarray], indices: list[int], clip_size: int):\n",
        "    \"\"\"Stacks a list of frames into a single numpy array\n",
        "\n",
        "    Args:\n",
        "        frames: List[T] of (H,W,3) float32\n",
        "        indices: List[T] of int\n",
        "\n",
        "    Returns:\n",
        "        (1,T,H,W,3) float32 in [0,1]\n",
        "\n",
        "    In a parallel/distributed groupby, a pre-group sort isnâ€™t guaranteed\n",
        "    to survive aggregation order; partitions can concatenate in\n",
        "    non-deterministic order. Additionally, the image dtype is natively a\n",
        "    list[uint8], so we need to cast to float32 before normalizing from\n",
        "    [0,255] to [0,1].\n",
        "\n",
        "    Steps:\n",
        "    1. Aggregate both image_tensor and frame_index.\n",
        "    2. Sort by frame_index inside the group-level UDF, then stack.\n",
        "    3. Normalize and cast in one step.\n",
        "    4. Add a batch dimension and return.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Don't assume frames are sorted already:\n",
        "    order = np.argsort(np.asarray(indices))\n",
        "\n",
        "    # Convert Daft Image to np.ndarray\n",
        "    def to_np(x):\n",
        "        if hasattr(x, \"to_numpy\"):\n",
        "            return x.to_numpy()          # Daft Image -> np.ndarray (H,W,C) uint8\n",
        "        return np.asarray(x)\n",
        "\n",
        "    # Sort frames by frame_index\n",
        "    frames_sorted = [to_np(frames[i]) for i in order]\n",
        "\n",
        "    # Ensure Tails are padded with duplicates\n",
        "    if len(order) < clip_size:\n",
        "        frames_sorted.extend([frames_sorted[-1]] * (clip_size - len(order)))\n",
        "\n",
        "    # Stack, Normalize, and Cast in one step\n",
        "    x = np.stack(frames_sorted[:clip_size], axis=0).astype(np.float32) / 255.0 # (T,H,W,3) float32 in [0,1]\n",
        "\n",
        "    return x # [1,T,H,W,C] where T=clip_size\n",
        "\n",
        "df_clips = df_grouped.with_column(\"clip\", stack_clip(df_grouped[\"data\"], df_grouped[\"frame_index\"], clip_size=T))\n",
        "df_clips.show(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f73ead73",
      "metadata": {
        "id": "f73ead73"
      },
      "source": [
        "### Define Inference Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c0dd8a",
      "metadata": {
        "id": "88c0dd8a"
      },
      "outputs": [],
      "source": [
        "@daft.udf(\n",
        "    return_dtype = dt.embedding(dt.float32(), 768),\n",
        "    batch_size=B, # clips per batch (tune for throughput)\n",
        "    num_gpus=1,\n",
        ")\n",
        "class VideoPrismVideoUDF:\n",
        "    def __init__(self, model_name: str = \"videoprism_lvt_public_v1_base\"):\n",
        "        \"for 'videoprism_lvt_public_v1_large', set T = 8\"\n",
        "\n",
        "        from videoprism import models as vp\n",
        "        self.model = vp.get_model(model_name)\n",
        "        self.params = vp.load_pretrained_weights(model_name)\n",
        "\n",
        "        @jax.jit\n",
        "        def vf_b(clips):  # [B,T,288,288,3] -> [B,D]\n",
        "            v, _, _ = self.model.apply(\n",
        "                self.params,\n",
        "                clips,\n",
        "                None,\n",
        "                None,\n",
        "                train=False\n",
        "            )\n",
        "            return v\n",
        "\n",
        "        self.vf_b = vf_b\n",
        "\n",
        "        # Warmup both\n",
        "        _ = self.vf_b(jnp.zeros((B, T, H, W, C), jnp.float32)).block_until_ready()\n",
        "\n",
        "    def __call__(self,\n",
        "        clips: list[np.ndarray], # List[T,H,W,C] of len B\n",
        "    ):\n",
        "        # Batch Inference\n",
        "        xb = jnp.stack(clips, axis=0) # [B,T,H,W,C]\n",
        "        video_embeddings = self.vf_b(xb) # [B,768]\n",
        "        np_embeddings = np.asarray(video_embeddings)  # Back to NumPy\n",
        "        return [np_embeddings[i].tolist() for i in range(B)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gNx0eBzCQV7R",
      "metadata": {
        "id": "gNx0eBzCQV7R"
      },
      "source": [
        "Previous runs with 24 batches of 16 frame clips processed in 128 sec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a5712b",
      "metadata": {
        "id": "e2a5712b"
      },
      "outputs": [],
      "source": [
        "print(f\"Video Embeddings will process {B} clips of {T} frame each at {W}x{H}x{3}\")\n",
        "\n",
        "df_clips_few = df_clips.sort(\"group_index\").collect()\n",
        "df_video_embs = df_clips_few.with_column(\"video_embeddings\", VideoPrismVideoUDF(df_clips_few[\"clip\"])).collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nb5h4MpIFbn0",
      "metadata": {
        "id": "nb5h4MpIFbn0"
      },
      "outputs": [],
      "source": [
        "df_video_embs.select(\"group_index\",\"video_embeddings\", \"clip\").count_rows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4691d0da",
      "metadata": {
        "id": "4691d0da"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be80a58",
      "metadata": {
        "id": "3be80a58"
      },
      "outputs": [],
      "source": [
        "# Parakeet Transcribe with Timestamps\n",
        "@daft.udf(return_dtype = dt.struct({\n",
        "    \"word\": dt.list(dt.struct({\n",
        "        \"word\": dt.string(),\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "    \"segment\": dt.list(dt.struct({\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "    \"char\": dt.list(dt.struct({\n",
        "        \"char\": dt.string(),\n",
        "        \"start_offset\": dt.int32(),\n",
        "        \"end_offset\": dt.int32(),\n",
        "        \"start\": dt.float32(),\n",
        "        \"end\": dt.float32()\n",
        "    })),\n",
        "}))\n",
        "class ParakeetTranscribeTimestampsUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        import nemo.collections.asr as nemo_asr\n",
        "        self.asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")\n",
        "        self.asr_model.change_attention_model(\n",
        "            self_attention_model=\"rel_pos_local_attn\",\n",
        "            att_context_size=[context_size, context_size]\n",
        "        )\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio, timestamps=True)   # No public flag to emit only segments\n",
        "        return [o.timestamp for o in outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ab5926",
      "metadata": {
        "id": "19ab5926"
      },
      "outputs": [],
      "source": [
        "class DiarizationSortFormerUDF:\n",
        "    def __init__(self, context_size: int = 256):\n",
        "        from nemo.collections.asr.models import SortformerEncLabelModel\n",
        "        self.diar_model = SortformerEncLabelModel.from_pretrained(\"nvidia/diar_streaming_sortformer_4spk-v2\")\n",
        "        self.diar_model.eval() # Switch to inference mode\n",
        "\n",
        "    def __call__(self, audio: list[np.ndarray]):\n",
        "        outputs = self.asr_model.transcribe(audio, timestamps=True)   # No public flag to emit only segments\n",
        "        return [o.timestamp for o in outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0e09c6",
      "metadata": {
        "id": "4d0e09c6"
      },
      "outputs": [],
      "source": [
        "import av\n",
        "\n",
        "probe_info = av.probe(\"/content/Near-100% GPU Utilizationï¼š Embedding Millions of Text Documents With Qwen3 [WI83lRzk7YE].mkv\")\n",
        "print(probe_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ljf7xZ-hXkl9",
      "metadata": {
        "id": "ljf7xZ-hXkl9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
